{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5McNPOlukgPY"
   },
   "outputs": [],
   "source": [
    "# Standard library modules\n",
    "import collections\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "# Third-party modules\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local modules\n",
    "# import your own modules here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rW69pATPbH90",
    "outputId": "9f822a2e-2506-4cb0-d4a1-50bd0031e49a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    " dev = \"cuda\"\n",
    "else:\n",
    " dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnZoQPY9QoOK"
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "k8czyBwRPOeM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bkcs/HDD/secBertClassifier/Untitled Folder/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = os.getcwd()+'/Untitled Folder/'\n",
    "data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vCmKpiWAaclt"
   },
   "outputs": [],
   "source": [
    "# X_train = pd.read_csv(data_folder+'X_train.csv')\n",
    "# X_test = pd.read_csv(data_folder+'X_test.csv')\n",
    "# X_val = pd.read_csv(data_folder+'X_val.csv')\n",
    "\n",
    "# y_train = pd.read_csv(data_folder+'y_train.csv').to_numpy()\n",
    "# y_test = pd.read_csv(data_folder+'y_test.csv').to_numpy()\n",
    "# y_val = pd.read_csv(data_folder+'y_val.csv').to_numpy()\n",
    "\n",
    "n_samples = 20\n",
    "X_train = pd.read_csv(data_folder+'X_train.csv').sample(n_samples)\n",
    "X_test = pd.read_csv(data_folder+'X_test.csv').sample(n_samples)\n",
    "X_val = pd.read_csv(data_folder+'X_val.csv').sample(n_samples)\n",
    "\n",
    "# y_train = pd.read_csv(data_folder+'y_train.csv').sample(n_samples)\n",
    "# y_test = pd.read_csv(data_folder+'y_test.csv').sample(n_samples)\n",
    "# y_val = pd.read_csv(data_folder+'y_val.csv').sample(n_samples)\n",
    "\n",
    "# X_train = pd.read_csv(data_folder+'X_train.csv').sample(n_samples).to_numpy()\n",
    "# X_test = pd.read_csv(data_folder+'X_test.csv').sample(n_samples).to_numpy()\n",
    "# X_val = pd.read_csv(data_folder+'X_val.csv').sample(n_samples).to_numpy()\n",
    "\n",
    "y_train = pd.read_csv(data_folder+'y_train.csv').sample(n_samples).to_numpy()\n",
    "y_test = pd.read_csv(data_folder+'y_test.csv').sample(n_samples).to_numpy()\n",
    "y_val = pd.read_csv(data_folder+'y_val.csv').sample(n_samples).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VfR86Y5pXXts"
   },
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "max_length = 512\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-04\n",
    "num_class = 4\n",
    "labels = ['Timestamp dependence', 'Outdated Solidity version', 'Frozen Ether', 'Delegatecall Injection']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSNtjsohQtQG"
   },
   "source": [
    "### Load pretrained model form huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ykA5O_MHk8gr"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizerFast\n",
    "\n",
    "secBertTokenizer = BertTokenizerFast.from_pretrained(\"jackaduma/SecBERT\", do_lower_case=True)\n",
    "secBertModel = BertModel.from_pretrained(\"jackaduma/SecBERT\", num_labels = num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "j6YPZaLbBQuq",
    "outputId": "faf6726a-cfee-4774-e58f-f995855c5f29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(514, 768)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DKTdG15cqIN6"
   },
   "outputs": [],
   "source": [
    "def freeze_k_layer(secBert, k=1):\n",
    "  for param in secBert.encoder.layer[0:k].parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1ni4OTv0ShcU"
   },
   "outputs": [],
   "source": [
    "freeze_k_layer(secBertModel, k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1i-ZKPoA_vhG"
   },
   "outputs": [],
   "source": [
    "class Branch(nn.Module):\n",
    "  def __init__(self, input_size, hidden1_size, hidden2_size, dropout, num_outputs):\n",
    "    super(Branch, self).__init__()\n",
    "\n",
    "    self.dense1 = nn.Linear(input_size, hidden1_size)\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "    self.dense2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "    self.dense3 = nn.Linear(hidden2_size, num_outputs)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out_dense1 = self.dense1(x)\n",
    "    out_dropout = self.dropout(out_dense1)\n",
    "    out_dense2 = self.dense2(out_dropout)\n",
    "    out_dense3 = self.dense3(out_dense2)\n",
    "\n",
    "    return out_dense3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Y0-Oz9-KWgG6"
   },
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, original_model, num_classes):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.original_model = original_model\n",
    "        self.branches = nn.ModuleList([Branch(1024, 128, 64, 0.1, 2) for _ in range(num_classes)])\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, tfidf_features=None ,labels=None):\n",
    "        out_bert = self.original_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooler_out = out_bert.pooler_output\n",
    "        tfidf_features = tfidf_features.view(tfidf_features.size(0), -1) # Reshae tensor to have 2 dimensionspe th\n",
    "\n",
    "        combin_features = torch.cat([pooler_out, tfidf_features], dim=1) # concat output of branches with tfidf_features\n",
    "        output_branches = [branch(combin_features) for branch in self.branches]\n",
    "        outputs = [self.activation(out_branch) for out_branch in output_branches]\n",
    "        \n",
    "        # apply softmax function for each branch\n",
    "        out_soft = [self.activation(out) for out in outputs]\n",
    "        out_soft_max_indices = [torch.argmax(out, dim=1) for out in out_soft]\n",
    "        out_soft_max_indices = torch.stack(out_soft_max_indices, dim=1)\n",
    "\n",
    "        return out_soft, out_soft_max_indices\n",
    "\n",
    "secBertClassifierMultilabel = BaseModel(original_model=secBertModel, num_classes=num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_HaLTY1Q3Yv"
   },
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jBpwSctGVSHD"
   },
   "outputs": [],
   "source": [
    "class OpcodeData(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.X = X.to_numpy()\n",
    "        self.targets = y\n",
    "        self.max_len = max_len\n",
    "        self.tfidf = TfidfVectorizer(max_features=256) # Initialize a TF-IDF vectorizer\n",
    "        self.matrix = self.tfidf.fit_transform(X['BYTECODE'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        values = self.X[index]\n",
    "        for value in values:\n",
    "            text = value\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        # Transform the text into TF-IDF features\n",
    "        tfidf_features = self.matrix[index].todense()\n",
    "\n",
    "        return {\n",
    "            'index': index,\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'tfidf_features': torch.tensor(tfidf_features, dtype=torch.float),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RD2VWqXpWwe0"
   },
   "outputs": [],
   "source": [
    "training_set = OpcodeData(X_train, y_train, secBertTokenizer, max_length)\n",
    "validating_set = OpcodeData(X_val, y_val, secBertTokenizer, max_length)\n",
    "testing_set = OpcodeData(X_test, y_test, secBertTokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3NVSRQxxXORN"
   },
   "outputs": [],
   "source": [
    "# Create generator for Dataset with BATCH_SIZE\n",
    "training_loader = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE)\n",
    "validating_loader = DataLoader(validating_set, batch_size=VALID_BATCH_SIZE)\n",
    "testing_loader = DataLoader(testing_set, batch_size=VALID_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-XqDi7tjc9st"
   },
   "outputs": [],
   "source": [
    "def save_classification(y_test, y_pred, out_dir, labels):\n",
    "  if isinstance(y_pred, np.ndarray) == False:\n",
    "    y_pred = y_pred.toarray()\n",
    "\n",
    "  def accuracy(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        numerator = sum(np.logical_and(y_true[i], y_pred[i]))\n",
    "        denominator = sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "        if denominator != 0:\n",
    "          temp += numerator / denominator\n",
    "    return temp / y_true.shape[0]\n",
    "\n",
    "  out = classification_report(y_test,y_pred, output_dict=True, target_names=labels)\n",
    "  total_support = out['samples avg']['support']\n",
    "\n",
    "  mr = accuracy_score(y_test, y_pred)\n",
    "  acc = accuracy(y_test,y_pred)\n",
    "  hm = hamming_loss(y_test, y_pred)\n",
    "\n",
    "  out['Exact Match Ratio'] = {'precision': mr, 'recall': mr, 'f1-score': mr, 'support': total_support}\n",
    "  out['Hamming Loss'] = {'precision': hm, 'recall': hm, 'f1-score': hm, 'support': total_support}\n",
    "  out['Accuracy'] = {'precision': acc, 'recall': acc, 'f1-score': acc, 'support': total_support}\n",
    "  out_df = pd.DataFrame(out).transpose()\n",
    "  print(out_df)\n",
    "\n",
    "  out_df.to_csv(out_dir)\n",
    "\n",
    "  return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tvaa8kYuQ_FT"
   },
   "source": [
    "### Create model and fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PbicX7vmZKJA"
   },
   "outputs": [],
   "source": [
    "def calculate_score(y_true, preds):\n",
    "    acc_score = accuracy_score(y_true, preds)\n",
    "\n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TW4-QSOZr7G1"
   },
   "outputs": [],
   "source": [
    "def get_misclassified_data(labels, preds, indices):\n",
    "  misclassify_data = {}\n",
    "  for i in range(len(labels)):\n",
    "    is_append = False\n",
    "    reject_label = np.array(labels[i])\n",
    "    for j in range(len(labels[i])):\n",
    "      if labels[i, j] != preds[i, j]:\n",
    "        reject_label[j] = 2 # reject label\n",
    "        is_append = True\n",
    "\n",
    "    if is_append:\n",
    "      x_train_index = indices[i]\n",
    "      misclassify_data[x_train_index] = np.array(reject_label)\n",
    "  return misclassify_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1rDvxaNEIn-w"
   },
   "outputs": [],
   "source": [
    "def train_steps(training_loader, model, loss_f, optimizer):\n",
    "    print('Training...')\n",
    "    training_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    train_acc = 0.\n",
    "    train_f1 = 0.\n",
    "    misclassify_train_data = {}\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(training_loader):\n",
    "        # push the batch to gpu\n",
    "        indices = batch['index'].numpy()\n",
    "        # print(\"indices\")\n",
    "        # print(indices)\n",
    "        ids = batch['ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        tfidf_features = batch['tfidf_features'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        preds, max_indices = model(ids, attention_mask=mask, token_type_ids=token_type_ids, tfidf_features=tfidf_features)\n",
    "        # print(targets.size())\n",
    "        # calculate the loss for each branch\n",
    "        losses = [loss_f(preds[i], targets[:, i]) for i in range(targets.shape[1])]\n",
    "        average_loss = sum(losses) / targets.shape[1]\n",
    "        training_loss += average_loss.item()\n",
    "\n",
    "        label_ids = targets.to('cpu').numpy()\n",
    "        max_indices = max_indices.detach().cpu().numpy()\n",
    "        acc_score = accuracy_score(label_ids, max_indices)\n",
    "        train_acc += acc_score\n",
    "\n",
    "        misclassify_data = get_misclassified_data(label_ids, max_indices, indices)\n",
    "        misclassify_train_data.update(misclassify_data)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        average_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = training_loss / nb_tr_steps\n",
    "    epoch_acc = train_acc / nb_tr_steps\n",
    "\n",
    "    return epoch_loss, epoch_acc, misclassify_train_data\n",
    "\n",
    "\n",
    "def evaluate_steps(validating_loader, model, loss_f):\n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(validating_loader):\n",
    "        # push the batch to gpu\n",
    "        ids = batch['ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        tfidf_features = batch['tfidf_features'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "\n",
    "        \n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds, max_indices = model(ids, attention_mask=mask, token_type_ids=token_type_ids, tfidf_features=tfidf_features)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            losses = [loss_f(preds[i], targets[:, i]) for i in range(targets.shape[1])]\n",
    "            average_loss = sum(losses) / targets.shape[1]\n",
    "            total_loss += average_loss.item()\n",
    "\n",
    "            max_indices = max_indices.detach().cpu().numpy()\n",
    "            print(\"max_indices\")\n",
    "            print(max_indices)\n",
    "            total_preds += list(max_indices)\n",
    "            print(\"targets\")\n",
    "            print(targets)\n",
    "            total_labels += targets.tolist()\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(validating_loader)\n",
    "    acc_score = accuracy_score(total_labels, total_preds)\n",
    "\n",
    "    return avg_loss, acc_score\n",
    "\n",
    "def predict(testing_loader, model):\n",
    "    print(\"\\nPredicting...\")\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(testing_loader):\n",
    "        # push the batch to gpu\n",
    "        ids = batch['ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        tfidf_features = batch['tfidf_features'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds, max_indices = model(ids, attention_mask=mask, token_type_ids=token_type_ids, tfidf_features=tfidf_features)\n",
    "\n",
    "            max_indices = max_indices.detach().cpu().numpy()\n",
    "            total_preds += list(max_indices)\n",
    "            total_labels += targets.tolist()\n",
    "\n",
    "    return total_labels, total_preds\n",
    "\n",
    "\n",
    "def train(epochs, model, optimizer, criterion, dataloader):\n",
    "  data_train_loader, data_val_loader = dataloader\n",
    "  # set initial loss to infinite\n",
    "  best_valid_loss = float('inf')\n",
    "  train_losses = []\n",
    "  valid_losses = []\n",
    "  train_accuracies = []\n",
    "  valid_accuracies = []\n",
    "  misclassify_train_data = {}\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    print('Epoch {}/{} '.format(epoch + 1, epochs))\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc, misclassify_train_steps_data = train_steps(data_train_loader, model, criterion, optimizer)\n",
    "    valid_loss, valid_acc = evaluate_steps(data_val_loader, model, criterion)\n",
    "\n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'secbert-escort.pt')\n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    valid_accuracies.append(valid_acc)\n",
    "    misclassify_train_data.update(misclassify_train_steps_data)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print('\\t loss={:.4f} \\t accuracy={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(train_loss, train_acc, valid_loss, valid_acc, elapsed_time))\n",
    "\n",
    "  return train_accuracies, valid_accuracies, train_losses, valid_losses, misclassify_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mt0cnLwyo1lz"
   },
   "outputs": [],
   "source": [
    "def plot_graph(epochs, train, valid, tittle):\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    plt.title(tittle)\n",
    "    plt.plot(list(np.arange(epochs) + 1) , train, label='train')\n",
    "    plt.plot(list(np.arange(epochs) + 1), valid, label='validation')\n",
    "    plt.xlabel('num_epochs', fontsize=12)\n",
    "    plt.ylabel('loss', fontsize=12)\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(f\"{tittle}_secbert-tfidf.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JpTODBJjrN74",
    "outputId": "3f04f3cb-52df-4acd-d196-7ee0d7dcf333"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BaseModel(\n",
       "    (original_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(52000, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(514, 768)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (branches): ModuleList(\n",
       "      (0-3): 4 x Branch(\n",
       "        (dense1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dense2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dense3): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (activation): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secBertClassifierMultilabel = nn.DataParallel(secBertClassifierMultilabel)\n",
    "secBertClassifierMultilabel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pBA6dtJApX_a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 \n",
      "Training...\n",
      "indices\n",
      "[0 1 2 3 4 5 6 7]\n",
      "torch.Size([8, 4])\n",
      "indices\n",
      "[ 8  9 10 11 12 13 14 15]\n",
      "torch.Size([8, 4])\n",
      "indices\n",
      "[16 17 18 19]\n",
      "torch.Size([4, 4])\n",
      "\n",
      "Evaluating...\n",
      "max_indices\n",
      "[[1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]]\n",
      "targets\n",
      "tensor([[0, 1, 1, 1],\n",
      "        [0, 1, 1, 1],\n",
      "        [0, 1, 1, 0],\n",
      "        [0, 1, 1, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [0, 0, 1, 1],\n",
      "        [0, 1, 1, 0],\n",
      "        [0, 0, 1, 1]], device='cuda:0')\n",
      "max_indices\n",
      "[[1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]]\n",
      "targets\n",
      "tensor([[0, 1, 1, 1],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [0, 1, 1, 1],\n",
      "        [0, 1, 1, 1],\n",
      "        [0, 1, 1, 0],\n",
      "        [0, 1, 0, 0]], device='cuda:0')\n",
      "max_indices\n",
      "[[1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]]\n",
      "targets\n",
      "tensor([[0, 0, 1, 0],\n",
      "        [0, 0, 1, 1],\n",
      "        [1, 1, 0, 0],\n",
      "        [0, 1, 1, 0]], device='cuda:0')\n",
      "\t loss=0.6950 \t accuracy=0.2500 \t val_loss=0.6930  \t val_acc=0.1500  \t time=3.67s\n"
     ]
    }
   ],
   "source": [
    "# Creating the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params = secBertClassifierMultilabel.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_accuracies, valid_accuracies, train_losses, valid_losses, misclassify_train_data = train(EPOCHS, secBertClassifierMultilabel, optimizer, criterion, (training_loader, validating_loader))\n",
    "df = pd.DataFrame.from_dict(misclassify_train_data, orient='index', columns=['Timestamp dependence', 'Outdated Solidity version', 'Frozen Ether', 'Delegatecall Injection'])\n",
    "df.index.name = 'X_train_index'\n",
    "df.to_csv(data_folder+'misclassified-data-tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the result of training process\n",
    "\"\"\"\n",
    "plot_graph(EPOCHS, train_losses, valid_losses, \"Train_Validation Loss\")\n",
    "plot_graph(EPOCHS, train_accuracies, valid_accuracies, \"Train_Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting...\n",
      "max_indices\n",
      "[[1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]]\n",
      "targets\n",
      "tensor([[0, 0, 1, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 1, 1, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [0, 0, 1, 1],\n",
      "        [0, 0, 1, 0]], device='cuda:0')\n",
      "max_indices\n",
      "[[1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]]\n",
      "targets\n",
      "tensor([[0, 0, 1, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [0, 1, 1, 0],\n",
      "        [0, 1, 1, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 1, 1, 1],\n",
      "        [0, 1, 1, 1],\n",
      "        [1, 1, 0, 0]], device='cuda:0')\n",
      "max_indices\n",
      "[[1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 0]]\n",
      "targets\n",
      "tensor([[0, 1, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 1, 1, 0],\n",
      "        [1, 1, 0, 0]], device='cuda:0')\n",
      "Predicting took 0.32 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # store the start time before calling predict\n",
    "total_labels, total_preds = predict(testing_loader, secBertClassifierMultilabel) # call predict and store the results\n",
    "end_time = time.time() # store the end time after predict returns\n",
    "elapsed_time = end_time - start_time # calculate the elapsed time in seconds\n",
    "print(f\"Predicting took {elapsed_time:.2f} seconds.\") # print the elapsed time in a formatted way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fJ_ZuWaNwWgE",
    "outputId": "ac2b41f5-c89b-4792-a9d9-ef9442c9c40e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score  support\n",
      "Timestamp dependence        0.300000  1.000000  0.461538      6.0\n",
      "Outdated Solidity version   0.700000  1.000000  0.823529     14.0\n",
      "Frozen Ether                0.000000  0.000000  0.000000     11.0\n",
      "Delegatecall Injection      0.000000  0.000000  0.000000      3.0\n",
      "micro avg                   0.500000  0.588235  0.540541     34.0\n",
      "macro avg                   0.250000  0.500000  0.321267     34.0\n",
      "weighted avg                0.341176  0.588235  0.420548     34.0\n",
      "samples avg                 0.500000  0.583333  0.523333     34.0\n",
      "Exact Match Ratio           0.250000  0.250000  0.250000     34.0\n",
      "Hamming Loss                0.425000  0.425000  0.425000     34.0\n",
      "Accuracy                    0.441667  0.441667  0.441667     34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bkcs/miniconda3/envs/secbert/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Timestamp dependence</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Outdated Solidity version</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frozen Ether</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delegatecall Injection</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.420548</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samples avg</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.523333</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exact Match Ratio</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming Loss</th>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           precision    recall  f1-score  support\n",
       "Timestamp dependence        0.300000  1.000000  0.461538      6.0\n",
       "Outdated Solidity version   0.700000  1.000000  0.823529     14.0\n",
       "Frozen Ether                0.000000  0.000000  0.000000     11.0\n",
       "Delegatecall Injection      0.000000  0.000000  0.000000      3.0\n",
       "micro avg                   0.500000  0.588235  0.540541     34.0\n",
       "macro avg                   0.250000  0.500000  0.321267     34.0\n",
       "weighted avg                0.341176  0.588235  0.420548     34.0\n",
       "samples avg                 0.500000  0.583333  0.523333     34.0\n",
       "Exact Match Ratio           0.250000  0.250000  0.250000     34.0\n",
       "Hamming Loss                0.425000  0.425000  0.425000     34.0\n",
       "Accuracy                    0.441667  0.441667  0.441667     34.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_classification(y_test=np.array(total_labels), y_pred=np.array(total_preds), labels=labels, out_dir='escort-secbert-max_lengt-tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(secBertClassifierMultilabel.state_dict(),'secBert_TF_IDF.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
