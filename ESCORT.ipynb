{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHzmrdy0f8Dk"
   },
   "source": [
    "This code implement the architecture for multilabel text classification in [paper](https://arxiv.org/pdf/2103.12607v1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2lOX1Sr0iSD"
   },
   "source": [
    "# Import and connect to gg drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29907,
     "status": "ok",
     "timestamp": 1694228488057,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "diMysT7u0l9M",
    "outputId": "a3482555-2342-4ceb-c52b-6b30c7c06794"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1694228488058,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "q-E2Mi_30otZ",
    "outputId": "57b62771-929a-46e1-9452-02a36fb11d81"
   },
   "outputs": [],
   "source": [
    "# cd /content/drive/MyDrive/lab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 7447,
     "status": "ok",
     "timestamp": 1694228495500,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "_fJlPHWD0qcl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1694228495501,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "ZlwZK-zl-Lfr",
    "outputId": "0eed464a-7619-4afb-f6f6-57974e1cf8f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    " dev = \"cuda:0\"\n",
    "else:\n",
    " dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1694228495501,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "cqJSks2dp6QU"
   },
   "outputs": [],
   "source": [
    "def save_classification(y_test, y_pred, out_dir, labels):\n",
    "  if isinstance(y_pred, np.ndarray) == False:\n",
    "    y_pred = y_pred.toarray()\n",
    "\n",
    "  def accuracy(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        numerator = sum(np.logical_and(y_true[i], y_pred[i]))\n",
    "        denominator = sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "        if denominator != 0:\n",
    "          temp += numerator / denominator\n",
    "    return temp / y_true.shape[0]\n",
    "\n",
    "  out = classification_report(y_test,y_pred, output_dict=True, target_names=labels)\n",
    "  total_support = out['samples avg']['support']\n",
    "\n",
    "  mr = accuracy_score(y_test, y_pred)\n",
    "  acc = accuracy(y_test,y_pred)\n",
    "  hm = hamming_loss(y_test, y_pred)\n",
    "\n",
    "  out['Exact Match Ratio'] = {'precision': mr, 'recall': mr, 'f1-score': mr, 'support': total_support}\n",
    "  out['Hamming Loss'] = {'precision': hm, 'recall': hm, 'f1-score': hm, 'support': total_support}\n",
    "  out['Accuracy'] = {'precision': acc, 'recall': acc, 'f1-score': acc, 'support': total_support}\n",
    "  out_df = pd.DataFrame(out).transpose()\n",
    "  print(out_df)\n",
    "\n",
    "  out_df.to_csv(out_dir)\n",
    "\n",
    "  return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAQOzGAG1XRr"
   },
   "source": [
    "# Extract and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 61609,
     "status": "ok",
     "timestamp": 1694228557106,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "jvjJgMdH1KTP"
   },
   "outputs": [],
   "source": [
    "data_folder = '/home/bkcs/HDD/secBertClassifier/Untitled Folder/'\n",
    "\n",
    "# def extract_file():\n",
    "#   file_zip = os.getcwd() + '/Data_Cleansing.zip'\n",
    "#   with zipfile.ZipFile(file_zip, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(path=data_folder)\n",
    "\n",
    "# extract_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1694228557107,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "pUyJQrpO2Npj",
    "outputId": "2201725f-e51a-4866-9454-4a39e5b864c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_val.csv',\n",
       " 'misclassified-outdate.csv',\n",
       " 'y_test.csv',\n",
       " 'X_train.csv',\n",
       " 'X_test.csv',\n",
       " 'secbert-escort.pt',\n",
       " 'y_val.csv',\n",
       " 'misclassified-data.csv',\n",
       " 'y_train.csv']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 74596,
     "status": "ok",
     "timestamp": 1694228631680,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "DETohp6i2trU"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(data_folder+'X_train.csv')['BYTECODE'].to_numpy()\n",
    "X_test = pd.read_csv(data_folder+'X_test.csv')['BYTECODE'].to_numpy()\n",
    "X_val = pd.read_csv(data_folder+'X_val.csv')['BYTECODE'].to_numpy()\n",
    "\n",
    "y_train = pd.read_csv(data_folder+'y_train.csv').to_numpy()\n",
    "y_test = pd.read_csv(data_folder+'y_test.csv').to_numpy()\n",
    "y_val = pd.read_csv(data_folder+'y_val.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122280,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "8wki3TAqGFYM"
   },
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, num_words=None, lower=True) -> None:\n",
    "        self.word_index = {}\n",
    "        self.word_counts = {}\n",
    "        self.num_words = num_words\n",
    "        self.split = \" \"\n",
    "        self.lower = lower\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        \"\"\"\n",
    "        create vocabulary\n",
    "\n",
    "        Args:\n",
    "            text: list of strings or list of list of strings\n",
    "        \"\"\"\n",
    "        for text in texts:\n",
    "            seq = self.text_to_word_sequence(text)\n",
    "            for w in seq:\n",
    "                if w in self.word_counts:\n",
    "                    self.word_counts[w] += 1\n",
    "\n",
    "                else:\n",
    "                    self.word_counts[w] = 1\n",
    "        vocab = self.word_counts.keys()\n",
    "        self.word_index = dict(zip(vocab, list(range(1, len(vocab) + 1))))\n",
    "\n",
    "    def text_to_word_sequence(self, input_text):\n",
    "        if self.lower == True:\n",
    "            input_text = input_text.lower()\n",
    "\n",
    "        seq = input_text.split(self.split)\n",
    "        return seq\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        return list(self.texts_to_sequences_generator(texts))\n",
    "\n",
    "    def texts_to_sequences_generator(self, texts):\n",
    "        for text in texts:\n",
    "            seq = self.text_to_word_sequence(text)\n",
    "            vect = []\n",
    "            for w in seq:\n",
    "                i = self.word_index.get(w)\n",
    "                vect.append(i)\n",
    "            yield vect\n",
    "\n",
    "def pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=None,\n",
    "    dtype=\"int32\",\n",
    "    padding=\"pre\",\n",
    "    truncating=\"pre\",\n",
    "    value=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: List of sequences (each sequence is a list of integers).\n",
    "        maxlen: Optional Int, maximum length of all sequences. If not provided,\n",
    "            sequences will be padded to the length of the longest individual\n",
    "            sequence.\n",
    "        dtype: (Optional, defaults to `\"int32\"`). Type of the output sequences.\n",
    "            To pad sequences with variable length strings, you can use `object`.\n",
    "        padding: String, \"pre\" or \"post\" (optional, defaults to `\"pre\"`):\n",
    "            pad either before or after each sequence.\n",
    "        truncating: String, \"pre\" or \"post\" (optional, defaults to `\"pre\"`):\n",
    "            remove values from sequences larger than\n",
    "            `maxlen`, either at the beginning or at the end of the sequences.\n",
    "        value: Float or String, padding value. (Optional, defaults to 0.)\n",
    "\n",
    "    Returns:\n",
    "        Numpy array with shape `(len(sequences), maxlen)`\n",
    "\n",
    "    Raises:\n",
    "        ValueError: In case of invalid values for `truncating` or `padding`,\n",
    "            or in case of invalid shape for a `sequences` entry.\n",
    "    \"\"\"\n",
    "\n",
    "    if not hasattr(sequences, \"__len__\"):\n",
    "        raise ValueError(\"`sequences` must be iterable.\")\n",
    "    num_samples = len(sequences)\n",
    "\n",
    "    lengths = []\n",
    "    sample_shape = ()\n",
    "    flag = True\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "\n",
    "    for x in sequences:\n",
    "        try:\n",
    "            lengths.append(len(x))\n",
    "            if flag and len(x):\n",
    "                sample_shape = np.asarray(x).shape[1:]\n",
    "                flag = False\n",
    "        except TypeError as e:\n",
    "            raise ValueError(\n",
    "                \"`sequences` must be a list of iterables. \"\n",
    "                f\"Found non-iterable: {str(x)}\"\n",
    "            ) from e\n",
    "\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(\n",
    "        dtype, np.unicode_\n",
    "    )\n",
    "    if isinstance(value, str) and dtype != object and not is_dtype_str:\n",
    "        raise ValueError(\n",
    "            f\"`dtype` {dtype} is not compatible with `value`'s type: \"\n",
    "            f\"{type(value)}\\nYou should set `dtype=object` for variable length \"\n",
    "            \"strings.\"\n",
    "        )\n",
    "\n",
    "    x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if not len(s):\n",
    "            continue  # empty list/array was found\n",
    "        if truncating == \"pre\":\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == \"post\":\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError(f'Truncating type \"{truncating}\" not understood')\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError(\n",
    "                f\"Shape of sample {trunc.shape[1:]} of sequence at \"\n",
    "                f\"position {idx} is different from expected shape \"\n",
    "                f\"{sample_shape}\"\n",
    "            )\n",
    "\n",
    "        if padding == \"post\":\n",
    "            x[idx, : len(trunc)] = trunc\n",
    "        elif padding == \"pre\":\n",
    "            x[idx, -len(trunc) :] = trunc\n",
    "        else:\n",
    "            raise ValueError(f'Padding type \"{padding}\" not understood')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "336ITdh_HNIW"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts=X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(texts=X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts=X_test)\n",
    "sequences_val = tokenizer.texts_to_sequences(texts=X_val)\n",
    "input_size = 17500\n",
    "\n",
    "X_train = pad_sequences(sequences_train, maxlen=input_size)\n",
    "X_val = pad_sequences(sequences_val, maxlen=input_size)\n",
    "X_test = pad_sequences(sequences_test, maxlen=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF9kUKTQ23ok"
   },
   "source": [
    "# Multilabel deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1q9IdP92n3xT"
   },
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4100])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([122280, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQHSbxdc_Ewh"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "tensor_X_train = torch.tensor(X_train)\n",
    "tensor_X_val = torch.tensor(X_val)\n",
    "tensor_X_test = torch.tensor(X_test)\n",
    "tensor_Y_train = torch.FloatTensor(y_train)\n",
    "tensor_Y_val = torch.FloatTensor(y_val)\n",
    "tensor_Y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "train_dataset = TensorDataset(tensor_X_train, tensor_Y_train)\n",
    "val_dataset = TensorDataset(tensor_X_val, tensor_Y_val)\n",
    "test_dataset = TensorDataset(tensor_X_test, tensor_Y_test)\n",
    "\n",
    "data_train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "data_val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "data_test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbYNVPGPu1gC"
   },
   "outputs": [],
   "source": [
    "class Branch(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, dropout, num_outputs):\n",
    "    super(Branch, self).__init__()\n",
    "\n",
    "    self.dense1 = nn.Linear(input_size, hidden_size)\n",
    "    self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "    self.dense2 = nn.Linear(hidden_size, num_outputs)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out_dense1 = self.dense1(x)\n",
    "    out_batchnorm1 = self.batchnorm1(out_dense1)\n",
    "    out_dropout = self.dropout(out_batchnorm1)\n",
    "    out_dense2 = self.dense2(out_dropout)\n",
    "\n",
    "    return out_dense2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiBchyFhCZvq"
   },
   "outputs": [],
   "source": [
    "# gru_hidden_size = 64\n",
    "# dense1_size = 128\n",
    "# dense2_size = 64\n",
    "# num_outputs = 1\n",
    "# embedd_size = 5\n",
    "\n",
    "# vocab = len(tokenizer.word_index.keys())\n",
    "# word_embeddings = nn.Embedding(input_size, embedd_size)\n",
    "# gru = nn.GRU(embedd_size, gru_hidden_size, num_layers=1)\n",
    "# branch_module = Branch(gru_hidden_size, dense1_size, dense2_size, num_outputs)\n",
    "# branches = nn.ModuleList([Branch(gru_hidden_size, dense1_size, dense2_size, num_outputs) for _ in range(num_classes)])\n",
    "# sigmoid = nn.Sigmoid()\n",
    "\n",
    "# inputs = next(iter(data_train_loader))\n",
    "# print(inputs[0])\n",
    "# print(inputs[0].shape)\n",
    "# print(inputs[1].shape)\n",
    "\n",
    "# embeds = word_embeddings(inputs[0])\n",
    "# gru_out, _ = gru(embeds)\n",
    "# outputs = [branch(gru_out[:, -1, :]) for branch in branches]\n",
    "# outputs = torch.cat(outputs, dim=1)\n",
    "# sigmoid(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvxJELKhn7ny"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N95V8a_125ys"
   },
   "outputs": [],
   "source": [
    "class Escort(nn.Module):\n",
    "  def __init__(self, vocab_size, embedd_size, gru_hidden_size, n_layers, num_classes):\n",
    "    super(Escort, self).__init__()\n",
    "    self.word_embeddings = nn.Embedding(vocab_size, embedd_size)\n",
    "    self.gru = nn.GRU(embedd_size, gru_hidden_size, num_layers=n_layers)\n",
    "    self.branches = nn.ModuleList([Branch(gru_hidden_size, 128, 64, 0.2, 1) for _ in range(num_classes)])\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, sequence):\n",
    "    embeds = self.word_embeddings(sequence)\n",
    "    gru_out, _ = self.gru(embeds)\n",
    "    output_branches = [branch(gru_out[:, -1, :]) for branch in self.branches]\n",
    "    output_branches = torch.cat(output_branches, dim=1)\n",
    "    outputs = self.sigmoid(output_branches)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BAJUkZcoAyE"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NW7noauGLMtU"
   },
   "outputs": [],
   "source": [
    "# model = LSTMMultilabel(SIZE_OF_VOCAB, NUM_HIDDEN_NODES, NUM_OUTPUT_NODES, NUM_LAYERS, DROPOUT, True, torch.DoubleTensor(weight))\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBfVRGCGMQxh"
   },
   "outputs": [],
   "source": [
    "# inputs, labels = next(iter(data_train_loader))\n",
    "# preds = model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vxHiIbwrKRS"
   },
   "source": [
    "### Train and Validation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UF8lqT-M-BL6"
   },
   "outputs": [],
   "source": [
    "def calculate_score(y_true, preds):\n",
    "    acc_score = accuracy_score(y_true, preds)\n",
    "\n",
    "    return acc_score\n",
    "\n",
    "def train_steps(training_loader, model, loss_f, optimizer):\n",
    "    training_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    train_acc = 0.\n",
    "\n",
    "    model.train()\n",
    "    for step, batch in enumerate(training_loader):\n",
    "        # push the batch to gpu\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "\n",
    "        preds = model(inputs)\n",
    "\n",
    "        loss = loss_f(preds, labels)\n",
    "        training_loss += loss.item()\n",
    "\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        preds = np.where(preds>=0.5, 1, 0)\n",
    "        labels = labels.to('cpu').numpy()\n",
    "\n",
    "        acc_score = calculate_score(labels, preds)\n",
    "        train_acc += acc_score\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = training_loss / nb_tr_steps\n",
    "    epoch_acc = train_acc / nb_tr_steps\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_steps(validating_loader, model, loss_f):\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(validating_loader):\n",
    "        # push the batch to gpu\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds = model(inputs)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = loss_f(preds, labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            preds = np.where(preds>=0.5, 1, 0)\n",
    "            total_preds += list(preds)\n",
    "            total_labels += labels.tolist()\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(validating_loader)\n",
    "    acc_score = calculate_score(total_labels, total_preds)\n",
    "\n",
    "    return avg_loss, acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZ9uzmMGsOhR"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQ82Aqtbpifd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(epochs, model, optimizer, criterion):\n",
    "  # empty lists to store training and validation loss of each epoch\n",
    "  # set initial loss to infinite\n",
    "  best_valid_loss = float('inf')\n",
    "  train_losses = []\n",
    "  valid_losses = []\n",
    "  train_accuracies = []\n",
    "  valid_accuracies = []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_steps(data_train_loader, model, criterion, optimizer)\n",
    "\n",
    "    valid_loss, valid_acc = evaluate_steps(data_val_loader, model, criterion)\n",
    "\n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './trained/escort.pt')\n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    valid_accuracies.append(valid_acc)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print('Epoch {}/{} \\t loss={:.4f} \\t accuracy={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(epoch + 1, epochs, train_loss, train_acc, valid_loss, valid_acc, elapsed_time))\n",
    "  return train_accuracies, valid_accuracies, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNXt80cVsWgx"
   },
   "outputs": [],
   "source": [
    "def plot_graph(epochs, train, valid, tittle):\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    plt.title(tittle)\n",
    "    plt.plot(list(np.arange(epochs) + 1) , train, label='train')\n",
    "    plt.plot(list(np.arange(epochs) + 1), valid, label='validation')\n",
    "    plt.xlabel('num_epochs', fontsize=12)\n",
    "    plt.ylabel('loss', fontsize=12)\n",
    "    plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXnRPRFDwxle"
   },
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soXxXY8yw3Er"
   },
   "outputs": [],
   "source": [
    "def predict(testing_loader, model, loss_f):\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    start_time = time.time()\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(testing_loader):\n",
    "        # push the batch to gpu\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds = model(inputs)\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            preds = np.where(preds>=0.5, 1, 0)\n",
    "            total_preds += list(preds)\n",
    "            total_labels += labels.tolist()\n",
    "\n",
    "    execution_time = (time.time() - start_time) / len(total_labels)\n",
    "    return total_preds, total_labels, execution_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9H2Jpz4Qx0MC"
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1694061109754,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "kr8ZtPtzx1VW",
    "outputId": "33abc99f-a97c-4aec-f38f-10c02df15b1b"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "SIZE_OF_VOCAB = len(tokenizer.word_index.keys())\n",
    "EMBEDDED_SIZE = 5\n",
    "GRU_HIDDEN_SIZE = 64\n",
    "NUM_OUTPUT_NODES = 4\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.2\n",
    "model = Escort(SIZE_OF_VOCAB, EMBEDDED_SIZE, GRU_HIDDEN_SIZE, NUM_LAYERS, NUM_OUTPUT_NODES)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64510,
     "status": "ok",
     "timestamp": 1694061177086,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "vTM0v9sK3nCZ",
    "outputId": "fac1f7c2-b792-47c0-e0da-37bbb86a8841"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "train_accuracies, valid_accuracies, train_losses, valid_losses = train(epochs, model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 892,
     "status": "ok",
     "timestamp": 1694061194264,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "n0ftnFQa3k34",
    "outputId": "5a83872f-22f8-4c9f-db11-613240211356"
   },
   "outputs": [],
   "source": [
    "plot_graph(epochs, train_losses, valid_losses, \"Train/Validation Loss\")\n",
    "plot_graph(epochs, train_accuracies, valid_accuracies, \"Train/Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pELUFirP3Kcx"
   },
   "outputs": [],
   "source": [
    "total_preds, total_labels, execution_time = predict(data_test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1694061194786,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "SkKf6FVD5NVW",
    "outputId": "c8bb6973-b009-4e6f-b387-9c56a46e1d85"
   },
   "outputs": [],
   "source": [
    "execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1694061196096,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "KoxRi-3x4JGY",
    "outputId": "24e519a0-38ee-4156-cd39-8e2a3f532d49"
   },
   "outputs": [],
   "source": [
    "save_classification(y_pred=np.array(total_preds), y_test=np.array(total_labels), labels=labels, out_dir='escort.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
