{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6476634,"sourceType":"datasetVersion","datasetId":3740786}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install gensim==3.4.0\n# !pip install smart_open==1.9.0","metadata":{"execution":{"iopub.status.busy":"2023-11-25T14:32:44.104022Z","iopub.execute_input":"2023-11-25T14:32:44.104341Z","iopub.status.idle":"2023-11-25T14:32:44.108428Z","shell.execute_reply.started":"2023-11-25T14:32:44.104315Z","shell.execute_reply":"2023-11-25T14:32:44.107574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport collections\nimport numpy as np\nimport zipfile\nimport time\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport time\nimport csv\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import *\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import train_test_split\n\nimport math\nfrom transformers import BertModel, BertTokenizerFast","metadata":{"execution":{"iopub.status.busy":"2023-11-25T14:32:44.112980Z","iopub.execute_input":"2023-11-25T14:32:44.113277Z","iopub.status.idle":"2023-11-25T14:33:05.306635Z","shell.execute_reply.started":"2023-11-25T14:32:44.113252Z","shell.execute_reply":"2023-11-25T14:33:05.305655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n dev = \"cuda\"\nelse:\n dev = \"cpu\"\ndevice = torch.device(dev)\ndevice","metadata":{"id":"rW69pATPbH90","outputId":"9f822a2e-2506-4cb0-d4a1-50bd0031e49a","execution":{"iopub.status.busy":"2023-11-25T14:33:05.308095Z","iopub.execute_input":"2023-11-25T14:33:05.308672Z","iopub.status.idle":"2023-11-25T14:33:05.378165Z","shell.execute_reply.started":"2023-11-25T14:33:05.308643Z","shell.execute_reply":"2023-11-25T14:33:05.377108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read data","metadata":{"id":"mnZoQPY9QoOK"}},{"cell_type":"code","source":"\n# def extract_file():\n#   file_zip = os.getcwd() + '/split-data.zip'\n#   with zipfile.ZipFile(file_zip, 'r') as zip_ref:\n#     zip_ref.extractall(path=data_folder)\n\n# extract_file()","metadata":{"id":"k8czyBwRPOeM","execution":{"iopub.status.busy":"2023-11-25T14:33:05.379558Z","iopub.execute_input":"2023-11-25T14:33:05.379986Z","iopub.status.idle":"2023-11-25T14:33:05.392241Z","shell.execute_reply.started":"2023-11-25T14:33:05.379957Z","shell.execute_reply":"2023-11-25T14:33:05.391380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"vCmKpiWAaclt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load pretrained model form huggingface","metadata":{"id":"HSNtjsohQtQG"}},{"cell_type":"code","source":"def freeze_k_layer(secBert, k=1):\n  for param in secBert.encoder.layer[0:k].parameters():\n    param.requires_grad = False","metadata":{"id":"DKTdG15cqIN6","execution":{"iopub.status.busy":"2023-11-25T14:33:05.394278Z","iopub.execute_input":"2023-11-25T14:33:05.394541Z","iopub.status.idle":"2023-11-25T14:33:05.406766Z","shell.execute_reply.started":"2023-11-25T14:33:05.394518Z","shell.execute_reply":"2023-11-25T14:33:05.405947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Branch(nn.Module):\n  def __init__(self, input_size, hidden_size, dropout, num_outputs):\n    super(Branch, self).__init__()\n\n    self.dense1 = nn.Linear(input_size, hidden_size)\n    self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n    self.dropout = nn.Dropout(p=dropout)\n    self.dense2 = nn.Linear(hidden_size, num_outputs)\n\n  def forward(self, x):\n    # print(\"Branch Input Shape:\", x.shape)\n    out_dense1 = self.dense1(x)\n    # print(\"After Dense1 Shape:\", out_dense1.shape)\n    out_batchnorm1 = self.batchnorm1(out_dense1)\n    out_dropout = self.dropout(out_batchnorm1)\n    out_dense2 = self.dense2(out_dropout)\n\n    return out_dense2","metadata":{"id":"1i-ZKPoA_vhG","execution":{"iopub.status.busy":"2023-11-25T14:33:05.407887Z","iopub.execute_input":"2023-11-25T14:33:05.408157Z","iopub.status.idle":"2023-11-25T14:33:05.420238Z","shell.execute_reply.started":"2023-11-25T14:33:05.408134Z","shell.execute_reply":"2023-11-25T14:33:05.419481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BaseModel(nn.Module):\n    def __init__(self, original_model, num_classes):\n        super(BaseModel, self).__init__()\n        self.num_classes = num_classes\n        self.original_model = original_model\n        self.branches = nn.ModuleList([Branch(1280, 128, 0.1, 2) for _ in range(num_classes)])\n        self.activation = nn.Softmax(dim=1)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, tfidf_features=None , word2vec = None ,labels=None):\n        out_bert = self.original_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        pooler_out = out_bert.pooler_output\n        tfidf_features = tfidf_features.view(tfidf_features.size(0), -1) # Reshae tensor to have 2 dimensionspe th\n        word2vec = word2vec.view(word2vec.size(0), -1)\n        combin_features = torch.cat([pooler_out, tfidf_features, word2vec], dim=1) # concat output of branches with tfidf_features\n        output_branches = [branch(combin_features) for branch in self.branches]\n        outputs = [self.activation(out_branch) for out_branch in output_branches]\n        \n        # apply softmax function for each branch\n        out_soft = [self.activation(out) for out in outputs]\n        out_soft_max_indices = [torch.argmax(out, dim=1) for out in out_soft]\n        out_soft_max_indices = torch.stack(out_soft_max_indices, dim=1)\n\n        return out_soft, out_soft_max_indices","metadata":{"id":"Y0-Oz9-KWgG6","execution":{"iopub.status.busy":"2023-11-25T14:33:05.421252Z","iopub.execute_input":"2023-11-25T14:33:05.421503Z","iopub.status.idle":"2023-11-25T14:33:05.436961Z","shell.execute_reply.started":"2023-11-25T14:33:05.421481Z","shell.execute_reply":"2023-11-25T14:33:05.436321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess data","metadata":{"id":"K_HaLTY1Q3Yv"}},{"cell_type":"code","source":"class OpcodeData(Dataset):\n    def __init__(self, X, y, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.X = X.to_numpy()\n        self.targets = y\n        self.max_len = max_len\n        self.tfidf = TfidfVectorizer(max_features=256) # Initialize a TF-IDF vectorizer\n        self.matrix = self.tfidf.fit_transform(X['BYTECODE'])\n        splits = []\n        for sentence in self.X:\n            for x in sentence:\n                l = x.split()\n            splits.append(l)\n        self.model = Word2Vec(splits,min_count=1, window=7,vector_size=256)\n        \n    def avg(self,text):\n        for x in text:\n            k = x.split()\n        word_vectors = [self.model.wv[word] for word in k]\n        return np.mean(word_vectors, axis=0)\n    \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, index):\n        word2vec = self.avg(self.X[index])\n        values = self.X[index]\n        for value in values:\n            text = value\n        inputs = self.tokenizer(\n            text,\n            None,\n            truncation=True,\n            padding='max_length',\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n        # Transform the text into TF-IDF features\n        tfidf_features = self.matrix[index]\n        tfidf_features = tfidf_features.todense()\n\n        return {\n            'index': index,\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'tfidf_features': torch.tensor(tfidf_features, dtype=torch.float),\n            'word2vec': torch.tensor(word2vec, dtype=torch.float),\n            'targets': torch.tensor(self.targets[index], dtype=torch.long)\n        }","metadata":{"id":"jBpwSctGVSHD","execution":{"iopub.status.busy":"2023-11-25T14:33:05.437943Z","iopub.execute_input":"2023-11-25T14:33:05.438195Z","iopub.status.idle":"2023-11-25T14:33:05.457617Z","shell.execute_reply.started":"2023-11-25T14:33:05.438172Z","shell.execute_reply":"2023-11-25T14:33:05.456781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_classification(y_test, y_pred, out_dir, labels):\n  if isinstance(y_pred, np.ndarray) == False:\n    y_pred = y_pred.toarray()\n\n  def accuracy(y_true, y_pred):\n    temp = 0\n    for i in range(y_true.shape[0]):\n        numerator = sum(np.logical_and(y_true[i], y_pred[i]))\n        denominator = sum(np.logical_or(y_true[i], y_pred[i]))\n        if denominator != 0:\n          temp += numerator / denominator\n    return temp / y_true.shape[0]\n\n  out = classification_report(y_test,y_pred, output_dict=True, target_names=labels)\n  total_support = out['samples avg']['support']\n\n  mr = accuracy_score(y_test, y_pred)\n  acc = accuracy(y_test,y_pred)\n  hm = hamming_loss(y_test, y_pred)\n\n  out['Exact Match Ratio'] = {'precision': mr, 'recall': mr, 'f1-score': mr, 'support': total_support}\n  out['Hamming Loss'] = {'precision': hm, 'recall': hm, 'f1-score': hm, 'support': total_support}\n  out['Accuracy'] = {'precision': acc, 'recall': acc, 'f1-score': acc, 'support': total_support}\n  out_df = pd.DataFrame(out).transpose()\n  print(out_df)\n\n  out_df.to_csv(out_dir)\n\n  return out_df","metadata":{"id":"-XqDi7tjc9st","execution":{"iopub.status.busy":"2023-11-25T14:33:05.458834Z","iopub.execute_input":"2023-11-25T14:33:05.459411Z","iopub.status.idle":"2023-11-25T14:33:05.476225Z","shell.execute_reply.started":"2023-11-25T14:33:05.459380Z","shell.execute_reply":"2023-11-25T14:33:05.475543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create model and fine-tuning","metadata":{"id":"Tvaa8kYuQ_FT"}},{"cell_type":"code","source":"def calculate_score(y_true, preds):\n    acc_score = accuracy_score(y_true, preds)\n\n    return acc_score","metadata":{"id":"PbicX7vmZKJA","execution":{"iopub.status.busy":"2023-11-25T14:33:05.477171Z","iopub.execute_input":"2023-11-25T14:33:05.477447Z","iopub.status.idle":"2023-11-25T14:33:05.494061Z","shell.execute_reply.started":"2023-11-25T14:33:05.477424Z","shell.execute_reply":"2023-11-25T14:33:05.493236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_misclassified_data(labels, preds, indices):\n  misclassify_data = {}\n  for i in range(len(labels)):\n    is_append = False\n    reject_label = np.array(labels[i])\n    for j in range(len(labels[i])):\n      if labels[i, j] != preds[i, j]:\n        reject_label[j] = 2 # reject label\n        is_append = True\n\n    if is_append:\n      x_train_index = indices[i]\n      misclassify_data[x_train_index] = np.array(reject_label)\n  return misclassify_data","metadata":{"id":"TW4-QSOZr7G1","execution":{"iopub.status.busy":"2023-11-25T14:33:05.497409Z","iopub.execute_input":"2023-11-25T14:33:05.497716Z","iopub.status.idle":"2023-11-25T14:33:05.507741Z","shell.execute_reply.started":"2023-11-25T14:33:05.497693Z","shell.execute_reply":"2023-11-25T14:33:05.507032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_steps(training_loader, model, loss_f, optimizer):\n    print('Training...')\n    training_loss = 0\n    n_correct = 0\n    nb_tr_steps = 0\n    nb_tr_examples = 0\n    train_acc = 0.\n    train_f1 = 0.\n    misclassify_train_data = {}\n\n    model.train()\n\n    for step, batch in enumerate(training_loader):\n        # push the batch to gpu\n        indices = batch['index'].numpy()\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        tfidf_features = batch['tfidf_features'].to(device)\n        word2vec = batch['word2vec'].to(device)\n        targets = batch['targets'].to(device)\n\n        preds, max_indices = model(ids, attention_mask=mask, token_type_ids=token_type_ids, tfidf_features=tfidf_features,word2vec=word2vec)\n\n        # calculate the loss for each branch\n        losses = [loss_f(preds[i], targets[:, i]) for i in range(targets.shape[1])]\n        average_loss = sum(losses) / targets.shape[1]\n        training_loss += average_loss.item()\n\n        label_ids = targets.to('cpu').numpy()\n        max_indices = max_indices.detach().cpu().numpy()\n        acc_score = accuracy_score(label_ids, max_indices)\n        train_acc += acc_score\n\n        misclassify_data = get_misclassified_data(label_ids, max_indices, indices)\n        misclassify_train_data.update(misclassify_data)\n\n        nb_tr_steps += 1\n\n        optimizer.zero_grad()\n        average_loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # When using GPU\n        optimizer.step()\n\n    epoch_loss = training_loss / nb_tr_steps\n    epoch_acc = train_acc / nb_tr_steps\n\n    return epoch_loss, epoch_acc, misclassify_train_data\n\n\ndef evaluate_steps(validating_loader, model, loss_f):\n    print(\"Evaluating...\")\n\n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save the model predictions\n    total_preds = []\n    total_labels = []\n    # iterate over batches\n    for step, batch in enumerate(validating_loader):\n        # push the batch to gpu\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        tfidf_features = batch['tfidf_features'].to(device)\n        word2vec = batch['word2vec'].to(device)\n        targets = batch['targets'].to(device)\n        \n        # deactivate autograd\n        with torch.no_grad():\n            # model predictions\n            preds, max_indices = model(ids, attention_mask=mask, token_type_ids=token_type_ids, tfidf_features=tfidf_features,word2vec=word2vec)\n\n            # compute the validation loss between actual and predicted values\n            losses = [loss_f(preds[i], targets[:, i]) for i in range(targets.shape[1])]\n            average_loss = sum(losses) / targets.shape[1]\n            total_loss += average_loss.item()\n\n            max_indices = max_indices.detach().cpu().numpy()\n            total_preds += list(max_indices)\n            total_labels += targets.tolist()\n    # compute the validation loss of the epoch\n    avg_loss = total_loss / len(validating_loader)\n    acc_score = accuracy_score(total_labels, total_preds)\n\n    return avg_loss, acc_score\n\ndef predict(testing_loader, model):\n    print(\"\\nPredicting...\")\n    # deactivate dropout layers\n    model.eval()\n\n    # empty list to save the model predictions\n    total_preds = []\n    total_labels = []\n    # iterate over batches\n    for step, batch in enumerate(testing_loader):\n        # push the batch to gpu\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        tfidf_features = batch['tfidf_features'].to(device)\n        word2vec = batch['word2vec'].to(device)\n        targets = batch['targets'].to(device)\n\n        # deactivate autograd\n        with torch.no_grad():\n            # model predictions\n            preds, max_indices = model(ids, attention_mask=mask, token_type_ids=token_type_ids, tfidf_features=tfidf_features,word2vec=word2vec)\n\n            max_indices = max_indices.detach().cpu().numpy()\n            total_preds += list(max_indices)\n            total_labels += targets.tolist()\n\n    return total_labels, total_preds\n\n\ndef train(epochs, model, optimizer, criterion, dataloader, save_model_dir):\n  data_train_loader, data_val_loader = dataloader\n  # set initial loss to infinite\n  best_valid_loss = float('inf')\n  train_losses = []\n  valid_losses = []\n  train_accuracies = []\n  valid_accuracies = []\n  misclassify_train_data = {}\n  total_time = 0.0\n\n  for epoch in range(epochs):\n    print('Epoch {}/{} '.format(epoch + 1, epochs))\n    start_time = time.time()\n    train_loss, train_acc, misclassify_train_steps_data = train_steps(data_train_loader, model, criterion, optimizer)\n    valid_loss, valid_acc = evaluate_steps(data_val_loader, model, criterion)\n\n    # save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), save_model_dir)\n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    train_accuracies.append(train_acc)\n    valid_accuracies.append(valid_acc)\n    misclassify_train_data.update(misclassify_train_steps_data)\n\n    elapsed_time = time.time() - start_time\n    total_time += elapsed_time\n\n    print('\\t loss={:.4f} \\t accuracy={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(train_loss, train_acc, valid_loss, valid_acc, elapsed_time))\n  print(f'Total time: {total_time}')\n  return train_accuracies, valid_accuracies, train_losses, valid_losses, misclassify_train_data","metadata":{"id":"1rDvxaNEIn-w","execution":{"iopub.status.busy":"2023-11-25T14:33:05.508943Z","iopub.execute_input":"2023-11-25T14:33:05.509304Z","iopub.status.idle":"2023-11-25T14:33:05.537796Z","shell.execute_reply.started":"2023-11-25T14:33:05.509267Z","shell.execute_reply":"2023-11-25T14:33:05.536839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_graph(epochs, train, valid, tittle):\n    fig = plt.figure(figsize=(12,12))\n    plt.title(tittle)\n    plt.plot(list(np.arange(epochs) + 1) , train, label='train')\n    plt.plot(list(np.arange(epochs) + 1), valid, label='validation')\n    plt.xlabel('num_epochs', fontsize=12)\n    plt.ylabel('loss', fontsize=12)\n    plt.legend(loc='best')\n    plt.savefig(f\"{tittle}.png\")","metadata":{"id":"mt0cnLwyo1lz","execution":{"iopub.status.busy":"2023-11-25T14:33:05.538794Z","iopub.execute_input":"2023-11-25T14:33:05.539069Z","iopub.status.idle":"2023-11-25T14:33:05.557758Z","shell.execute_reply.started":"2023-11-25T14:33:05.539045Z","shell.execute_reply":"2023-11-25T14:33:05.557070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining some key variables that will be used later on in the training\nmax_length = 512\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 32\nEPOCHS = 20\nLEARNING_RATE = 1e-04\nnum_class = 4\nlabels = ['Timestamp dependence', 'Outdated Solidity version', 'Frozen Ether', 'Delegatecall Injection']\ndata_folder = '/kaggle/input/opcode/'\nout_folder = '/kaggle/working/'\nsave_model_dir = out_folder + 'secbert-escort-tfidf-w2v.pt'\nmis_classified_data_file = out_folder+'misclassified-data-tfidf-w2v.csv'\nreport_dir = out_folder + 'secbert-escort-tfidf-w2v.csv'","metadata":{"execution":{"iopub.status.busy":"2023-11-25T14:33:05.558807Z","iopub.execute_input":"2023-11-25T14:33:05.559108Z","iopub.status.idle":"2023-11-25T14:33:05.571834Z","shell.execute_reply.started":"2023-11-25T14:33:05.559082Z","shell.execute_reply":"2023-11-25T14:33:05.571159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.read_csv(data_folder+'X_train.csv')\nX_test = pd.read_csv(data_folder+'X_test.csv')\nX_val = pd.read_csv(data_folder+'X_val.csv')\n\ny_train = pd.read_csv(data_folder+'y_train.csv').to_numpy()\ny_test = pd.read_csv(data_folder+'y_test.csv').to_numpy()\ny_val = pd.read_csv(data_folder+'y_val.csv').to_numpy()","metadata":{"id":"JpTODBJjrN74","outputId":"3f04f3cb-52df-4acd-d196-7ee0d7dcf333","execution":{"iopub.status.busy":"2023-11-25T14:33:05.572906Z","iopub.execute_input":"2023-11-25T14:33:05.573184Z","iopub.status.idle":"2023-11-25T14:34:41.411032Z","shell.execute_reply.started":"2023-11-25T14:33:05.573160Z","shell.execute_reply":"2023-11-25T14:34:41.409989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"secBertTokenizer = BertTokenizerFast.from_pretrained(\"jackaduma/SecBERT\", do_lower_case=True)\nsecBertModel = BertModel.from_pretrained(\"jackaduma/SecBERT\", num_labels = num_class)\nfreeze_k_layer(secBertModel, k=6)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T14:34:41.412496Z","iopub.execute_input":"2023-11-25T14:34:41.413264Z","iopub.status.idle":"2023-11-25T14:34:45.670407Z","shell.execute_reply.started":"2023-11-25T14:34:41.413222Z","shell.execute_reply":"2023-11-25T14:34:45.669331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set = OpcodeData(X_train, y_train, secBertTokenizer, max_length)\nvalidating_set = OpcodeData(X_val, y_val, secBertTokenizer, max_length)\ntesting_set = OpcodeData(X_test, y_test, secBertTokenizer, max_length)\n# Create generator for Dataset with BATCH_SIZE\ntraining_loader = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE)\nvalidating_loader = DataLoader(validating_set, batch_size=VALID_BATCH_SIZE)\ntesting_loader = DataLoader(testing_set, batch_size=VALID_BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T14:34:45.679571Z","iopub.execute_input":"2023-11-25T14:34:45.680315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"secBertClassifierMultilabel = BaseModel(original_model=secBertModel, num_classes=num_class)\nsecBertClassifierMultilabel = nn.DataParallel(secBertClassifierMultilabel)\nsecBertClassifierMultilabel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params = secBertClassifierMultilabel.parameters(), lr=LEARNING_RATE)\n\ntrain_accuracies, valid_accuracies, train_losses, valid_losses, misclassify_train_data = train(EPOCHS, secBertClassifierMultilabel, optimizer, criterion, (training_loader, validating_loader))\ndf = pd.DataFrame.from_dict(misclassify_train_data, orient='index', columns=['Timestamp dependence', 'Outdated Solidity version', 'Frozen Ether', 'Delegatecall Injection'])\ndf.index.name = 'X_train_index'\ndf.to_csv(mis_classified_data_file)","metadata":{"id":"pBA6dtJApX_a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nPlot the result of training process\n\"\"\"\nplot_graph(EPOCHS, train_losses, valid_losses, \"Train_Validation_Loss_secbert-tfidf-w2v\")\nplot_graph(EPOCHS, train_accuracies, valid_accuracies, \"Train_Validation_Accuracy_secbert-tfidf-w2v\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time() # store the start time before calling predict\ntotal_labels, total_preds = predict(testing_loader, secBertClassifierMultilabel) # call predict and store the results\nend_time = time.time() # store the end time after predict returns\nelapsed_time = end_time - start_time # calculate the elapsed time in seconds\nprint(f\"Predicting took {elapsed_time} seconds.\") # print the elapsed time in a formatted way","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_classification(y_test=np.array(total_labels), y_pred=np.array(total_preds), labels=labels, out_dir=report_dir)","metadata":{"id":"fJ_ZuWaNwWgE","outputId":"ac2b41f5-c89b-4792-a9d9-ef9442c9c40e","trusted":true},"execution_count":null,"outputs":[]}]}