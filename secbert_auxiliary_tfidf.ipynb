{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:32:44.113277Z","iopub.status.busy":"2023-11-25T14:32:44.112980Z","iopub.status.idle":"2023-11-25T14:33:05.306635Z","shell.execute_reply":"2023-11-25T14:33:05.305655Z","shell.execute_reply.started":"2023-11-25T14:32:44.113252Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","import collections\n","import numpy as np\n","import zipfile\n","import time\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","import time\n","import csv\n","import torch\n","import torch.nn as nn\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import *\n","from sklearn.model_selection import train_test_split\n","\n","import math\n","from transformers import BertModel, BertTokenizerFast"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:33:05.308672Z","iopub.status.busy":"2023-11-25T14:33:05.308095Z","iopub.status.idle":"2023-11-25T14:33:05.378165Z","shell.execute_reply":"2023-11-25T14:33:05.377108Z","shell.execute_reply.started":"2023-11-25T14:33:05.308643Z"},"id":"rW69pATPbH90","outputId":"9f822a2e-2506-4cb0-d4a1-50bd0031e49a","trusted":true},"outputs":[],"source":["if torch.cuda.is_available():\n"," dev = \"cuda\"\n","else:\n"," dev = \"cpu\"\n","device = torch.device(dev)\n","device"]},{"cell_type":"markdown","metadata":{"id":"mnZoQPY9QoOK"},"source":["### Read data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:33:05.379986Z","iopub.status.busy":"2023-11-25T14:33:05.379558Z","iopub.status.idle":"2023-11-25T14:33:05.392241Z","shell.execute_reply":"2023-11-25T14:33:05.391380Z","shell.execute_reply.started":"2023-11-25T14:33:05.379957Z"},"id":"k8czyBwRPOeM","trusted":true},"outputs":[],"source":["\n","# def extract_file():\n","#   file_zip = os.getcwd() + '/split-data.zip'\n","#   with zipfile.ZipFile(file_zip, 'r') as zip_ref:\n","#     zip_ref.extractall(path=data_folder)\n","\n","# extract_file()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCmKpiWAaclt","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"HSNtjsohQtQG"},"source":["### Load pretrained model form huggingface"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:33:05.394541Z","iopub.status.busy":"2023-11-25T14:33:05.394278Z","iopub.status.idle":"2023-11-25T14:33:05.406766Z","shell.execute_reply":"2023-11-25T14:33:05.405947Z","shell.execute_reply.started":"2023-11-25T14:33:05.394518Z"},"id":"DKTdG15cqIN6","trusted":true},"outputs":[],"source":["def freeze_k_layer(secBert, k=1):\n","  for param in secBert.encoder.layer[0:k].parameters():\n","    param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:33:05.408157Z","iopub.status.busy":"2023-11-25T14:33:05.407887Z","iopub.status.idle":"2023-11-25T14:33:05.420238Z","shell.execute_reply":"2023-11-25T14:33:05.419481Z","shell.execute_reply.started":"2023-11-25T14:33:05.408134Z"},"id":"1i-ZKPoA_vhG","trusted":true},"outputs":[],"source":["class Branch(nn.Module):\n","  def __init__(self, input_size, hidden_size, dropout, num_outputs):\n","    super(Branch, self).__init__()\n","\n","    self.dense1 = nn.Linear(input_size, hidden_size)\n","    self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n","    self.dropout = nn.Dropout(p=dropout)\n","    self.dense2 = nn.Linear(hidden_size, num_outputs)\n","\n","  def forward(self, x):\n","    # print(\"Branch Input Shape:\", x.shape)\n","    out_dense1 = self.dense1(x)\n","    # print(\"After Dense1 Shape:\", out_dense1.shape)\n","    out_batchnorm1 = self.batchnorm1(out_dense1)\n","    out_dropout = self.dropout(out_batchnorm1)\n","    out_dense2 = self.dense2(out_dropout)\n","\n","    return out_dense2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:33:05.421503Z","iopub.status.busy":"2023-11-25T14:33:05.421252Z","iopub.status.idle":"2023-11-25T14:33:05.436961Z","shell.execute_reply":"2023-11-25T14:33:05.436321Z","shell.execute_reply.started":"2023-11-25T14:33:05.421481Z"},"id":"Y0-Oz9-KWgG6","trusted":true},"outputs":[],"source":["class BaseModel(nn.Module):\n","    def __init__(self, original_model, num_classes):\n","        super(BaseModel, self).__init__()\n","        self.num_classes = num_classes\n","        self.original_model = original_model\n","        self.branches = nn.ModuleList([Branch(1024, 128, 0.1, 2) for _ in range(num_classes)])\n","        self.activation = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None, tfidf_features=None, labels=None):\n","        out_bert = self.original_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        pooler_out = out_bert.pooler_output\n","        tfidf_features = tfidf_features.view(tfidf_features.size(0), -1) # Reshae tensor to have 2 dimensionspe th\n","        combin_features = torch.cat([pooler_out, tfidf_features], dim=1) # concat output of branches with tfidf_features\n","        output_branches = [branch(combin_features) for branch in self.branches]\n","        outputs = [self.activation(out_branch) for out_branch in output_branches]\n","        \n","        # apply softmax function for each branch\n","        out_soft = [self.activation(out) for out in outputs]\n","        out_soft_max_indices = [torch.argmax(out, dim=1) for out in out_soft]\n","        out_soft_max_indices = torch.stack(out_soft_max_indices, dim=1)\n","\n","        return out_soft, out_soft_max_indices"]},{"cell_type":"markdown","metadata":{"id":"K_HaLTY1Q3Yv"},"source":["### Preprocess data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:33:05.438195Z","iopub.status.busy":"2023-11-25T14:33:05.437943Z","iopub.status.idle":"2023-11-25T14:33:05.457617Z","shell.execute_reply":"2023-11-25T14:33:05.456781Z","shell.execute_reply.started":"2023-11-25T14:33:05.438172Z"},"id":"jBpwSctGVSHD","trusted":true},"outputs":[],"source":["class OpcodeData(Dataset):\n","    def __init__(self, X, y, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.X = X.to_numpy()\n","        self.targets = y\n","        self.max_len = max_len\n","        self.tfidf = TfidfVectorizer(max_features=256) # Initialize a TF-IDF vectorizer\n","        self.matrix = self.tfidf.fit_transform(X['BYTECODE'])\n","        splits = []\n","        for sentence in self.X:\n","            for x in sentence:\n","                l = x.split()\n","            splits.append(l)\n","        \n","    def avg(self,text):\n","        for x in text:\n","            k = x.split()\n","        word_vectors = [self.model.wv[word] for word in k]\n","        return np.mean(word_vectors, axis=0)\n","    \n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, index):\n","        values = self.X[index]\n","        for value in values:\n","            text = value\n","        inputs = self.tokenizer(\n","            text,\n","            None,\n","            truncation=True,\n","            padding='max_length',\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","        # Transform the text into TF-IDF features\n","        tfidf_features = self.matrix[index]\n","        tfidf_features = tfidf_features.todense()\n","\n","        return {\n","            'index': index,\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'tfidf_features': torch.tensor(tfidf_features, dtype=torch.float),\n","            'targets': torch.tensor(self.targets[index], dtype=torch.long)\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:33:05.459411Z","iopub.status.busy":"2023-11-25T14:33:05.458834Z","iopub.status.idle":"2023-11-25T14:33:05.476225Z","shell.execute_reply":"2023-11-25T14:33:05.475543Z","shell.execute_reply.started":"2023-11-25T14:33:05.459380Z"},"id":"-XqDi7tjc9st","trusted":true},"outputs":[],"source":["def save_classification(y_test, y_pred, out_dir, labels):\n","  if isinstance(y_pred, np.ndarray) == False:\n","    y_pred = y_pred.toarray()\n","\n","  def accuracy(y_true, y_pred):\n","    temp = 0\n","    for i in range(y_true.shape[0]):\n","        numerator = sum(np.logical_and(y_true[i], y_pred[i]))\n","        denominator = sum(np.logical_or(y_true[i], y_pred[i]))\n","        if denominator != 0:\n","          temp += numerator / denominator\n","    return temp / y_true.shape[0]\n","\n","  out = classification_report(y_test,y_pred, output_dict=True, target_names=labels)\n","  total_support = out['samples avg']['support']\n","\n","  mr = accuracy_score(y_test, y_pred)\n","  acc = accuracy(y_test,y_pred)\n","  hm = hamming_loss(y_test, y_pred)\n","\n","  out['Exact Match Ratio'] = {'precision': mr, 'recall': mr, 'f1-score': mr, 'support': total_support}\n","  out['Hamming Loss'] = {'precision': hm, 'recall': hm, 'f1-score': hm, 'support': total_support}\n","  out['Accuracy'] = {'precision': acc, 'recall': acc, 'f1-score': acc, 'support': total_support}\n","  out_df = pd.DataFrame(out).transpose()\n","  print(out_df)\n","\n","  out_df.to_csv(out_dir)\n","\n","  return out_df"]},{"cell_type":"markdown","metadata":{"id":"Tvaa8kYuQ_FT"},"source":["### Create model and fine-tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:33:05.477447Z","iopub.status.busy":"2023-11-25T14:33:05.477171Z","iopub.status.idle":"2023-11-25T14:33:05.494061Z","shell.execute_reply":"2023-11-25T14:33:05.493236Z","shell.execute_reply.started":"2023-11-25T14:33:05.477424Z"},"id":"PbicX7vmZKJA","trusted":true},"outputs":[],"source":["def calculate_score(y_true, preds):\n","    acc_score = accuracy_score(y_true, preds)\n","\n","    return acc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:33:05.497716Z","iopub.status.busy":"2023-11-25T14:33:05.497409Z","iopub.status.idle":"2023-11-25T14:33:05.507741Z","shell.execute_reply":"2023-11-25T14:33:05.507032Z","shell.execute_reply.started":"2023-11-25T14:33:05.497693Z"},"id":"TW4-QSOZr7G1","trusted":true},"outputs":[],"source":["def get_misclassified_data(labels, preds, indices):\n","  misclassify_data = {}\n","  for i in range(len(labels)):\n","    is_append = False\n","    reject_label = np.array(labels[i])\n","    for j in range(len(labels[i])):\n","      if labels[i, j] != preds[i, j]:\n","        reject_label[j] = 2 # reject label\n","        is_append = True\n","\n","    if is_append:\n","      x_train_index = indices[i]\n","      misclassify_data[x_train_index] = np.array(reject_label)\n","  return misclassify_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:33:05.509304Z","iopub.status.busy":"2023-11-25T14:33:05.508943Z","iopub.status.idle":"2023-11-25T14:33:05.537796Z","shell.execute_reply":"2023-11-25T14:33:05.536839Z","shell.execute_reply.started":"2023-11-25T14:33:05.509267Z"},"id":"1rDvxaNEIn-w","trusted":true},"outputs":[],"source":["def train_steps(training_loader, model, loss_f, optimizer):\n","    print('Training...')\n","    training_loss = 0\n","    n_correct = 0\n","    nb_tr_steps = 0\n","    nb_tr_examples = 0\n","    train_acc = 0.\n","    train_f1 = 0.\n","    misclassify_train_data = {}\n","\n","    model.train()\n","\n","    for step, batch in enumerate(training_loader):\n","        # push the batch to gpu\n","        indices = batch['index'].numpy()\n","        ids = batch['ids'].to(device)\n","        mask = batch['mask'].to(device)\n","        token_type_ids = batch['token_type_ids'].to(device)\n","        tfidf_features = batch['tfidf_features'].to(device)\n","        targets = batch['targets'].to(device)\n","\n","        preds, max_indices = model(ids, attention_mask=mask, token_type_ids=token_type_ids, tfidf_features=tfidf_features)\n","\n","        # calculate the loss for each branch\n","        losses = [loss_f(preds[i], targets[:, i]) for i in range(targets.shape[1])]\n","        average_loss = sum(losses) / targets.shape[1]\n","        training_loss += average_loss.item()\n","\n","        label_ids = targets.to('cpu').numpy()\n","        max_indices = max_indices.detach().cpu().numpy()\n","        acc_score = accuracy_score(label_ids, max_indices)\n","        train_acc += acc_score\n","\n","        misclassify_data = get_misclassified_data(label_ids, max_indices, indices)\n","        misclassify_train_data.update(misclassify_data)\n","\n","        nb_tr_steps += 1\n","\n","        optimizer.zero_grad()\n","        average_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # When using GPU\n","        optimizer.step()\n","\n","    epoch_loss = training_loss / nb_tr_steps\n","    epoch_acc = train_acc / nb_tr_steps \n","\n","    return epoch_loss, epoch_acc, misclassify_train_data\n","\n","\n","def evaluate_steps(validating_loader, model, loss_f):\n","    print(\"Evaluating...\")\n","\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","    total_labels = []\n","    # iterate over batches\n","    for step, batch in enumerate(validating_loader):\n","        # push the batch to gpu\n","        ids = batch['ids'].to(device)\n","        mask = batch['mask'].to(device)\n","        token_type_ids = batch['token_type_ids'].to(device)\n","        tfidf_features = batch['tfidf_features'].to(device)\n","        targets = batch['targets'].to(device)\n","        \n","        # deactivate autograd\n","        with torch.no_grad():\n","            # model predictions\n","            preds, max_indices = model(ids, attention_mask=mask, token_type_ids=token_type_ids, tfidf_features=tfidf_features)\n","\n","            # compute the validation loss between actual and predicted values\n","            losses = [loss_f(preds[i], targets[:, i]) for i in range(targets.shape[1])]\n","            average_loss = sum(losses) / targets.shape[1]\n","            total_loss += average_loss.item()\n","\n","            max_indices = max_indices.detach().cpu().numpy()\n","            total_preds += list(max_indices)\n","            total_labels += targets.tolist()\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(validating_loader)\n","    acc_score = accuracy_score(total_labels, total_preds)\n","\n","    return avg_loss, acc_score\n","\n","def predict(testing_loader, model):\n","    print(\"\\nPredicting...\")\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","    total_labels = []\n","    # iterate over batches\n","    for step, batch in enumerate(testing_loader):\n","        # push the batch to gpu\n","        ids = batch['ids'].to(device)\n","        mask = batch['mask'].to(device)\n","        token_type_ids = batch['token_type_ids'].to(device)\n","        tfidf_features = batch['tfidf_features'].to(device)\n","        targets = batch['targets'].to(device)\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","            # model predictions\n","            preds, max_indices = model(ids, attention_mask=mask, token_type_ids=token_type_ids, tfidf_features=tfidf_features)\n","\n","            max_indices = max_indices.detach().cpu().numpy()\n","            total_preds += list(max_indices)\n","            total_labels += targets.tolist()\n","\n","    return total_labels, total_preds\n","\n","\n","def train(epochs, model, optimizer, criterion, dataloader, save_model_dir):\n","  data_train_loader, data_val_loader = dataloader\n","  # set initial loss to infinite\n","  best_valid_loss = float('inf')\n","  train_losses = []\n","  valid_losses = []\n","  train_accuracies = []\n","  valid_accuracies = []\n","  misclassify_train_data = {}\n","  total_time = 0.0\n","\n","  for epoch in range(epochs):\n","    print('Epoch {}/{} '.format(epoch + 1, epochs))\n","    start_time = time.time()\n","    train_loss, train_acc, misclassify_train_steps_data = train_steps(data_train_loader, model, criterion, optimizer)\n","    valid_loss, valid_acc = evaluate_steps(data_val_loader, model, criterion)\n","\n","    # save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), save_model_dir)\n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    train_accuracies.append(train_acc)\n","    valid_accuracies.append(valid_acc)\n","    misclassify_train_data.update(misclassify_train_steps_data)\n","\n","    elapsed_time = time.time() - start_time\n","    total_time += elapsed_time\n","\n","    print('\\t loss={:.4f} \\t accuracy={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(train_loss, train_acc, valid_loss, valid_acc, elapsed_time))\n","  print(f'Total time: {total_time}')\n","  return train_accuracies, valid_accuracies, train_losses, valid_losses, misclassify_train_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:33:05.539069Z","iopub.status.busy":"2023-11-25T14:33:05.538794Z","iopub.status.idle":"2023-11-25T14:33:05.557758Z","shell.execute_reply":"2023-11-25T14:33:05.557070Z","shell.execute_reply.started":"2023-11-25T14:33:05.539045Z"},"id":"mt0cnLwyo1lz","trusted":true},"outputs":[],"source":["def plot_graph(epochs, train, valid, tittle):\n","    fig = plt.figure(figsize=(12,12))\n","    plt.title(tittle)\n","    plt.plot(list(np.arange(epochs) + 1) , train, label='train')\n","    plt.plot(list(np.arange(epochs) + 1), valid, label='validation')\n","    plt.xlabel('num_epochs', fontsize=12)\n","    plt.ylabel('loss', fontsize=12)\n","    plt.legend(loc='best')\n","    plt.savefig(f\"{tittle}.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:33:05.559108Z","iopub.status.busy":"2023-11-25T14:33:05.558807Z","iopub.status.idle":"2023-11-25T14:33:05.571834Z","shell.execute_reply":"2023-11-25T14:33:05.571159Z","shell.execute_reply.started":"2023-11-25T14:33:05.559082Z"},"trusted":true},"outputs":[],"source":["# Defining some key variables that will be used later on in the training\n","\n","\n","max_length = 512\n","TRAIN_BATCH_SIZE = 32\n","VALID_BATCH_SIZE = 32\n","EPOCHS = 20\n","LEARNING_RATE = 1e-04\n","num_class = 4\n","labels = ['Timestamp dependence', 'Outdated Solidity version', 'Frozen Ether', 'Delegatecall Injection']\n","\n","# Getting the current directory of the script\n","current_dir = os.path.dirname(__file__)\n","\n","# Using relative paths for the data and output folders\n","data_folder = os.path.join(current_dir, 'Untitled Folder')\n","out_folder = os.path.join(current_dir, 'working')\n","\n","# Creating the folders if they do not exist\n","os.makedirs(data_folder, exist_ok=True)\n","os.makedirs(out_folder, exist_ok=True)\n","\n","# Using relative paths for the files\n","save_model_dir = os.path.join(out_folder, 'secbert-escort-tfidf-new.pt')\n","mis_classified_data_file = os.path.join(out_folder, 'misclassified-data-tfidf-new.csv')\n","report_dir = os.path.join(out_folder, 'secbert-escort-tfidf-new.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:33:05.573184Z","iopub.status.busy":"2023-11-25T14:33:05.572906Z","iopub.status.idle":"2023-11-25T14:34:41.411032Z","shell.execute_reply":"2023-11-25T14:34:41.409989Z","shell.execute_reply.started":"2023-11-25T14:33:05.573160Z"},"id":"JpTODBJjrN74","outputId":"3f04f3cb-52df-4acd-d196-7ee0d7dcf333","trusted":true},"outputs":[],"source":["X_train = pd.read_csv(data_folder+'X_train.csv')\n","X_test = pd.read_csv(data_folder+'X_test.csv')\n","X_val = pd.read_csv(data_folder+'X_val.csv')\n","\n","y_train = pd.read_csv(data_folder+'y_train.csv').to_numpy()\n","y_test = pd.read_csv(data_folder+'y_test.csv').to_numpy()\n","y_val = pd.read_csv(data_folder+'y_val.csv').to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:34:41.413264Z","iopub.status.busy":"2023-11-25T14:34:41.412496Z","iopub.status.idle":"2023-11-25T14:34:45.670407Z","shell.execute_reply":"2023-11-25T14:34:45.669331Z","shell.execute_reply.started":"2023-11-25T14:34:41.413222Z"},"trusted":true},"outputs":[],"source":["secBertTokenizer = BertTokenizerFast.from_pretrained(\"jackaduma/SecBERT\", do_lower_case=True)\n","secBertModel = BertModel.from_pretrained(\"jackaduma/SecBERT\", num_labels = num_class)\n","freeze_k_layer(secBertModel, k=6)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-25T14:34:45.680315Z","iopub.status.busy":"2023-11-25T14:34:45.679571Z"},"trusted":true},"outputs":[],"source":["training_set = OpcodeData(X_train, y_train, secBertTokenizer, max_length)\n","validating_set = OpcodeData(X_val, y_val, secBertTokenizer, max_length)\n","testing_set = OpcodeData(X_test, y_test, secBertTokenizer, max_length)\n","# Create generator for Dataset with BATCH_SIZE\n","training_loader = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE)\n","validating_loader = DataLoader(validating_set, batch_size=VALID_BATCH_SIZE)\n","testing_loader = DataLoader(testing_set, batch_size=VALID_BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["secBertClassifierMultilabel = BaseModel(original_model=secBertModel, num_classes=num_class)\n","secBertClassifierMultilabel = nn.DataParallel(secBertClassifierMultilabel)\n","secBertClassifierMultilabel.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBA6dtJApX_a","trusted":true},"outputs":[],"source":["# Creating the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params = secBertClassifierMultilabel.parameters(), lr=LEARNING_RATE)\n","\n","train_accuracies, valid_accuracies, train_losses, valid_losses, misclassify_train_data = train(EPOCHS, secBertClassifierMultilabel, optimizer, criterion, (training_loader, validating_loader))\n","df = pd.DataFrame.from_dict(misclassify_train_data, orient='index', columns=['Timestamp dependence', 'Outdated Solidity version', 'Frozen Ether', 'Delegatecall Injection'])\n","df.index.name = 'X_train_index'\n","df.to_csv(mis_classified_data_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","Plot the result of training process\n","\"\"\"\n","plot_graph(EPOCHS, train_losses, valid_losses, \"Train_Validation_Loss_secbert-tfidf-w2v\")\n","plot_graph(EPOCHS, train_accuracies, valid_accuracies, \"Train_Validation_Accuracy_secbert-tfidf-w2v\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["start_time = time.time() # store the start time before calling predict\n","total_labels, total_preds = predict(testing_loader, secBertClassifierMultilabel) # call predict and store the results\n","end_time = time.time() # store the end time after predict returns\n","elapsed_time = end_time - start_time # calculate the elapsed time in seconds\n","print(f\"Predicting took {elapsed_time} seconds.\") # print the elapsed time in a formatted way"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJ_ZuWaNwWgE","outputId":"ac2b41f5-c89b-4792-a9d9-ef9442c9c40e","trusted":true},"outputs":[],"source":["save_classification(y_test=np.array(total_labels), y_pred=np.array(total_preds), labels=labels, out_dir=report_dir)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3740786,"sourceId":6476634,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
