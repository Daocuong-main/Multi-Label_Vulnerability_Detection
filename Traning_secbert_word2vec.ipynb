{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T02:30:20.476893Z",
     "iopub.status.busy": "2023-09-15T02:30:20.476578Z",
     "iopub.status.idle": "2023-09-15T02:30:23.988029Z",
     "shell.execute_reply": "2023-09-15T02:30:23.986893Z",
     "shell.execute_reply.started": "2023-09-15T02:30:20.476862Z"
    },
    "id": "5McNPOlukgPY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import time\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "matplotlib.use('Agg')\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T02:30:23.990877Z",
     "iopub.status.busy": "2023-09-15T02:30:23.990436Z",
     "iopub.status.idle": "2023-09-15T02:30:24.021340Z",
     "shell.execute_reply": "2023-09-15T02:30:24.020353Z",
     "shell.execute_reply.started": "2023-09-15T02:30:23.990848Z"
    },
    "id": "rW69pATPbH90",
    "outputId": "9f822a2e-2506-4cb0-d4a1-50bd0031e49a"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    " dev = \"cuda\"\n",
    "else:\n",
    " dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnZoQPY9QoOK"
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8czyBwRPOeM"
   },
   "outputs": [],
   "source": [
    "data_folder = os.getcwd()+'/Untitled Folder/'\n",
    "data_folder\n",
    "# def extract_file():\n",
    "#   file_zip = os.getcwd() + '/split-data.zip'\n",
    "#   with zipfile.ZipFile(file_zip, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(path=data_folder)\n",
    "\n",
    "# extract_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCmKpiWAaclt"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(data_folder+'X_train.csv').to_numpy()\n",
    "X_test = pd.read_csv(data_folder+'X_test.csv').to_numpy()\n",
    "X_val = pd.read_csv(data_folder+'X_val.csv').to_numpy()\n",
    "\n",
    "y_train = pd.read_csv(data_folder+'y_train.csv').to_numpy()\n",
    "y_test = pd.read_csv(data_folder+'y_test.csv').to_numpy()\n",
    "y_val = pd.read_csv(data_folder+'y_val.csv').to_numpy()\n",
    "\n",
    "# n_samples = 20\n",
    "\n",
    "# X_train = pd.read_csv(data_folder+'X_train.csv').sample(n_samples).to_numpy()\n",
    "# X_test = pd.read_csv(data_folder+'X_test.csv').sample(n_samples).to_numpy()\n",
    "# X_val = pd.read_csv(data_folder+'X_val.csv').sample(n_samples).to_numpy()\n",
    "\n",
    "# y_train = pd.read_csv(data_folder+'y_train.csv').sample(n_samples).to_numpy()\n",
    "# y_test = pd.read_csv(data_folder+'y_test.csv').sample(n_samples).to_numpy()\n",
    "# y_val = pd.read_csv(data_folder+'y_val.csv').sample(n_samples).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfR86Y5pXXts"
   },
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "max_length = 512\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-04\n",
    "num_class = 4\n",
    "labels = ['Timestamp dependence', 'Outdated Solidity version', 'Frozen Ether', 'Delegatecall Injection']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSNtjsohQtQG"
   },
   "source": [
    "### Load pretrained model form huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykA5O_MHk8gr"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizerFast\n",
    "\n",
    "secBertTokenizer = BertTokenizerFast.from_pretrained(\"jackaduma/SecBERT\", do_lower_case=True)\n",
    "secBertModel = BertModel.from_pretrained(\"jackaduma/SecBERT\", num_labels = num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6YPZaLbBQuq",
    "outputId": "faf6726a-cfee-4774-e58f-f995855c5f29"
   },
   "outputs": [],
   "source": [
    "secBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKTdG15cqIN6"
   },
   "outputs": [],
   "source": [
    "def freeze_k_layer(secBert, k=1):\n",
    "  for param in secBert.encoder.layer[0:k].parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ni4OTv0ShcU"
   },
   "outputs": [],
   "source": [
    "freeze_k_layer(secBertModel, k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1i-ZKPoA_vhG"
   },
   "outputs": [],
   "source": [
    "class Branch(nn.Module):\n",
    "  def __init__(self, input_size, hidden1_size, hidden2_size, dropout, num_outputs):\n",
    "    super(Branch, self).__init__()\n",
    "\n",
    "    self.dense1 = nn.Linear(input_size, hidden1_size)\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "    self.dense2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "    self.dense3 = nn.Linear(hidden2_size, num_outputs)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out_dense1 = self.dense1(x)\n",
    "    out_dropout = self.dropout(out_dense1)\n",
    "    out_dense2 = self.dense2(out_dropout)\n",
    "    out_dense3 = self.dense3(out_dense2)\n",
    "\n",
    "    return out_dense3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0-Oz9-KWgG6"
   },
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, original_model, num_classes):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.original_model = original_model\n",
    "        self.branches = nn.ModuleList([Branch(1024, 128, 64, 0.1, 2) for _ in range(num_classes)])\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, w2v_features=None ,labels=None):\n",
    "        out_bert = self.original_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooler_out = out_bert.pooler_output\n",
    "        w2v_features = w2v_features.view(w2v_features.size(0), -1) # Reshape the tensor to have 2 dimensions\n",
    "        combin_features = torch.cat([pooler_out, w2v_features], dim=1) \n",
    "        output_branches = [branch(combin_features) for branch in self.branches]\n",
    "        outputs = [self.activation(out_branch) for out_branch in output_branches]\n",
    "        \n",
    "        # apply softmax function for each branch\n",
    "        out_soft = [self.activation(out) for out in outputs]\n",
    "        out_soft_max_indices = [torch.argmax(out, dim=1) for out in out_soft]\n",
    "        out_soft_max_indices = torch.stack(out_soft_max_indices, dim=1)\n",
    "\n",
    "        return out_soft, out_soft_max_indices\n",
    "\n",
    "secBertClassifierMultilabel = BaseModel(original_model=secBertModel, num_classes=num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_HaLTY1Q3Yv"
   },
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBpwSctGVSHD"
   },
   "outputs": [],
   "source": [
    "class OpcodeData(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.X = X\n",
    "        self.targets = y\n",
    "        self.max_len = max_len\n",
    "        splits = []\n",
    "        for sentence in X:\n",
    "            for x in sentence:\n",
    "                l = x.split()\n",
    "            splits.append(l)\n",
    "        self.model = Word2Vec(splits,min_count=3, window=7,vector_size=256)\n",
    "    \n",
    "    def avg(self,text):\n",
    "        for x in text:\n",
    "            k = x.split()\n",
    "        word_vectors = [self.model.wv[word] for word in k]\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        values = self.X[index]\n",
    "        for value in values:\n",
    "            text = value\n",
    "        word2vec = self.avg(self.X[index])\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'index': index,\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'word2vec': torch.tensor(word2vec, dtype=torch.float),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RD2VWqXpWwe0"
   },
   "outputs": [],
   "source": [
    "training_set = OpcodeData(X_train, y_train, secBertTokenizer, max_length)\n",
    "validating_set = OpcodeData(X_val, y_val, secBertTokenizer, max_length)\n",
    "testing_set = OpcodeData(X_test, y_test, secBertTokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NVSRQxxXORN"
   },
   "outputs": [],
   "source": [
    "# Create generator for Dataset with BATCH_SIZE\n",
    "training_loader = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE)\n",
    "validating_loader = DataLoader(validating_set, batch_size=VALID_BATCH_SIZE)\n",
    "testing_loader = DataLoader(testing_set, batch_size=VALID_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XqDi7tjc9st"
   },
   "outputs": [],
   "source": [
    "def save_classification(y_test, y_pred, out_dir, labels):\n",
    "  if isinstance(y_pred, np.ndarray) == False:\n",
    "    y_pred = y_pred.toarray()\n",
    "\n",
    "  def accuracy(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        numerator = sum(np.logical_and(y_true[i], y_pred[i]))\n",
    "        denominator = sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "        if denominator != 0:\n",
    "          temp += numerator / denominator\n",
    "    return temp / y_true.shape[0]\n",
    "\n",
    "  out = classification_report(y_test,y_pred, output_dict=True, target_names=labels)\n",
    "  total_support = out['samples avg']['support']\n",
    "\n",
    "  mr = accuracy_score(y_test, y_pred)\n",
    "  acc = accuracy(y_test,y_pred)\n",
    "  hm = hamming_loss(y_test, y_pred)\n",
    "\n",
    "  out['Exact Match Ratio'] = {'precision': mr, 'recall': mr, 'f1-score': mr, 'support': total_support}\n",
    "  out['Hamming Loss'] = {'precision': hm, 'recall': hm, 'f1-score': hm, 'support': total_support}\n",
    "  out['Accuracy'] = {'precision': acc, 'recall': acc, 'f1-score': acc, 'support': total_support}\n",
    "  out_df = pd.DataFrame(out).transpose()\n",
    "  print(out_df)\n",
    "\n",
    "  out_df.to_csv(out_dir)\n",
    "\n",
    "  return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tvaa8kYuQ_FT"
   },
   "source": [
    "### Create model and fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbicX7vmZKJA"
   },
   "outputs": [],
   "source": [
    "def calculate_score(y_true, preds):\n",
    "    acc_score = accuracy_score(y_true, preds)\n",
    "\n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TW4-QSOZr7G1"
   },
   "outputs": [],
   "source": [
    "def get_misclassified_data(labels, preds, indices):\n",
    "  misclassify_data = {}\n",
    "  for i in range(len(labels)):\n",
    "    is_append = False\n",
    "    reject_label = np.array(labels[i])\n",
    "    for j in range(len(labels[i])):\n",
    "      if labels[i, j] != preds[i, j]:\n",
    "        reject_label[j] = 2 # reject label\n",
    "        is_append = True\n",
    "\n",
    "    if is_append:\n",
    "      x_train_index = indices[i]\n",
    "      misclassify_data[x_train_index] = np.array(reject_label)\n",
    "  return misclassify_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rDvxaNEIn-w"
   },
   "outputs": [],
   "source": [
    "def train_steps(training_loader, model, loss_f, optimizer):\n",
    "    print('Training...')\n",
    "    training_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    train_acc = 0.\n",
    "    train_f1 = 0.\n",
    "    misclassify_train_data = {}\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(training_loader):\n",
    "        # push the batch to gpu\n",
    "        indices = batch['index'].numpy()\n",
    "        ids = batch['ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        w2v_features = batch['word2vec'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        preds, max_indices = model(ids, attention_mask=mask, token_type_ids=token_type_ids, w2v_features=w2v_features)\n",
    "\n",
    "        # calculate the loss for each branch\n",
    "        losses = [loss_f(preds[i], targets[:, i]) for i in range(targets.shape[1])]\n",
    "        average_loss = sum(losses) / targets.shape[1]\n",
    "        training_loss += average_loss.item()\n",
    "\n",
    "        label_ids = targets.to('cpu').numpy()\n",
    "        max_indices = max_indices.detach().cpu().numpy()\n",
    "        acc_score = accuracy_score(label_ids, max_indices)\n",
    "        train_acc += acc_score\n",
    "\n",
    "        misclassify_data = get_misclassified_data(label_ids, max_indices, indices)\n",
    "        misclassify_train_data.update(misclassify_data)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        average_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = training_loss / nb_tr_steps\n",
    "    epoch_acc = train_acc / nb_tr_steps\n",
    "\n",
    "    return epoch_loss, epoch_acc, misclassify_train_data\n",
    "\n",
    "\n",
    "def evaluate_steps(validating_loader, model, loss_f):\n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(validating_loader):\n",
    "        # push the batch to gpu\n",
    "        ids = batch['ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        w2v_features = batch['word2vec'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds, max_indices = model(ids, attention_mask=mask, token_type_ids=token_type_ids, w2v_features=w2v_features)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            losses = [loss_f(preds[i], targets[:, i]) for i in range(targets.shape[1])]\n",
    "            average_loss = sum(losses) / targets.shape[1]\n",
    "            total_loss += average_loss.item()\n",
    "\n",
    "            max_indices = max_indices.detach().cpu().numpy()\n",
    "            total_preds += list(max_indices)\n",
    "            total_labels += targets.tolist()\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(validating_loader)\n",
    "    acc_score = accuracy_score(total_labels, total_preds)\n",
    "\n",
    "    return avg_loss, acc_score\n",
    "\n",
    "def predict(testing_loader, model):\n",
    "    print(\"\\nPredicting...\")\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(testing_loader):\n",
    "        # push the batch to gpu\n",
    "        ids = batch['ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        w2v_features = batch['word2vec'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds, max_indices = model(ids, attention_mask=mask, token_type_ids=token_type_ids, w2v_features=w2v_features)\n",
    "\n",
    "            max_indices = max_indices.detach().cpu().numpy()\n",
    "            total_preds += list(max_indices)\n",
    "            total_labels += targets.tolist()\n",
    "\n",
    "    return total_labels, total_preds\n",
    "\n",
    "\n",
    "def train(epochs, model, optimizer, criterion, dataloader):\n",
    "  data_train_loader, data_val_loader = dataloader\n",
    "  # set initial loss to infinite\n",
    "  best_valid_loss = float('inf')\n",
    "  train_losses = []\n",
    "  valid_losses = []\n",
    "  train_accuracies = []\n",
    "  valid_accuracies = []\n",
    "  misclassify_train_data = {}\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    print('Epoch {}/{} '.format(epoch + 1, epochs))\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc, misclassify_train_steps_data = train_steps(data_train_loader, model, criterion, optimizer)\n",
    "    valid_loss, valid_acc = evaluate_steps(data_val_loader, model, criterion)\n",
    "\n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'secbert-escort.pt')\n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    valid_accuracies.append(valid_acc)\n",
    "    misclassify_train_data.update(misclassify_train_steps_data)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print('\\t loss={:.4f} \\t accuracy={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(train_loss, train_acc, valid_loss, valid_acc, elapsed_time))\n",
    "\n",
    "  return train_accuracies, valid_accuracies, train_losses, valid_losses, misclassify_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mt0cnLwyo1lz"
   },
   "outputs": [],
   "source": [
    "def plot_graph(epochs, train, valid, tittle):\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    plt.title(tittle)\n",
    "    plt.plot(list(np.arange(epochs) + 1) , train, label='train')\n",
    "    plt.plot(list(np.arange(epochs) + 1), valid, label='validation')\n",
    "    plt.xlabel('num_epochs', fontsize=12)\n",
    "    plt.ylabel('loss', fontsize=12)\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(f\"{tittle}_secbert-w2v.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpTODBJjrN74",
    "outputId": "3f04f3cb-52df-4acd-d196-7ee0d7dcf333"
   },
   "outputs": [],
   "source": [
    "secBertClassifierMultilabel = nn.DataParallel(secBertClassifierMultilabel)\n",
    "secBertClassifierMultilabel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBA6dtJApX_a"
   },
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params = secBertClassifierMultilabel.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_accuracies, valid_accuracies, train_losses, valid_losses, misclassify_train_data = train(EPOCHS, secBertClassifierMultilabel, optimizer, criterion, (training_loader, validating_loader))\n",
    "df = pd.DataFrame.from_dict(misclassify_train_data, orient='index', columns=['Timestamp dependence', 'Outdated Solidity version', 'Frozen Ether', 'Delegatecall Injection'])\n",
    "df.index.name = 'X_train_index'\n",
    "df.to_csv(data_folder+'misclassified-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the result of training process\n",
    "\"\"\"\n",
    "plot_graph(EPOCHS, train_losses, valid_losses, \"Train_Validation Loss\")\n",
    "plot_graph(EPOCHS, train_accuracies, valid_accuracies, \"Train_Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate model on test set and save the result\n",
    "\"\"\"\n",
    "total_labels, total_preds = predict(testing_loader, secBertClassifierMultilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJ_ZuWaNwWgE",
    "outputId": "ac2b41f5-c89b-4792-a9d9-ef9442c9c40e"
   },
   "outputs": [],
   "source": [
    "save_classification(y_test=np.array(total_labels), y_pred=np.array(total_preds), labels=labels, out_dir='escort-secbert-w2v.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(secBertClassifierMultilabel.state_dict(),'secBert_word2vec.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
