{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20064,"status":"ok","timestamp":1697860318469,"user":{"displayName":"Van Tong","userId":"03366431140678629004"},"user_tz":-420},"id":"I_qKuCi1r7FH","outputId":"04606dc5-6fe4-4956-8442-8240cd00320f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":835,"status":"ok","timestamp":1697860319297,"user":{"displayName":"Van Tong","userId":"03366431140678629004"},"user_tz":-420},"id":"B8m8NM_UFa2U","outputId":"5a672e52-bc76-4c0a-9243-693b4178d6cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Oct 21 03:51:59 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   63C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"bycwhRGu4Ndu","executionInfo":{"status":"ok","timestamp":1697860319297,"user_tz":-420,"elapsed":6,"user":{"displayName":"Van Tong","userId":"03366431140678629004"}}},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3Huq9BwU4RJw","executionInfo":{"status":"ok","timestamp":1697860319298,"user_tz":-420,"elapsed":6,"user":{"displayName":"Van Tong","userId":"03366431140678629004"}}},"outputs":[],"source":["# cd /content/drive/MyDrive/lab/"]},{"cell_type":"markdown","metadata":{"id":"yFLRYmcW241l"},"source":["# Import package"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5McNPOlukgPY","executionInfo":{"status":"ok","timestamp":1697860324291,"user_tz":-420,"elapsed":4999,"user":{"displayName":"Van Tong","userId":"03366431140678629004"}}},"outputs":[],"source":["import pandas as pd\n","import os\n","import collections\n","import numpy as np\n","import zipfile\n","import time\n","import matplotlib.pyplot as plt\n","import time\n","\n","import torch\n","import torch.nn as nn\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","\n","from sklearn.metrics import *\n","from sklearn.model_selection import train_test_split\n","\n","import math"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1697860324292,"user":{"displayName":"Van Tong","userId":"03366431140678629004"},"user_tz":-420},"id":"rW69pATPbH90","outputId":"2b5cb194-a8d2-4eab-9e35-60ffb7111416"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":6}],"source":["if torch.cuda.is_available():\n"," dev = \"cuda\"\n","else:\n"," dev = \"cpu\"\n","device = torch.device(dev)\n","device"]},{"cell_type":"markdown","metadata":{"id":"mnZoQPY9QoOK"},"source":["# Read data"]},{"cell_type":"markdown","metadata":{"id":"HSNtjsohQtQG"},"source":["# Define Model"]},{"cell_type":"markdown","metadata":{"id":"oMACVGN63Kni"},"source":["## Load pretrained model form huggingface"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ozEo6q0FozD5","executionInfo":{"status":"ok","timestamp":1697860324812,"user_tz":-420,"elapsed":525,"user":{"displayName":"Van Tong","userId":"03366431140678629004"}}},"outputs":[],"source":["num_class = 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EnLPpep6ozD6"},"outputs":[],"source":["from transformers import DistilBertTokenizer, DistilBertModel\n","\n","distilBertTokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n","distilBertModel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbgwMT7fBqCx"},"outputs":[],"source":["# from transformers import BertModel, BertTokenizerFast\n","\n","# secBertTokenizer = BertTokenizerFast.from_pretrained(\"jackaduma/SecBERT\", do_lower_case=True)\n","# secBertModel = BertModel.from_pretrained(\"jackaduma/SecBERT\", num_labels = 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bREyDX4ABiVq"},"outputs":[],"source":["distilBertModel"]},{"cell_type":"markdown","metadata":{"id":"uu3To2tr8CeW"},"source":["## Freeze layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKTdG15cqIN6"},"outputs":[],"source":["# freeze all layer in DistilBERT\n","def freeze_DistilBert_layer(DistilBert):\n","  for param in DistilBert.transformer.parameters():\n","    param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkAGPiA0Tbsr"},"outputs":[],"source":["# Freeze all branch except the last layer\n","def freeze_branch(branches):\n","  for branch in branches:\n","    branch.dense1.weight.requires_grad = False\n","    branch.dense1.bias.requires_grad = False\n","    branch.dense2.weight.requires_grad = False\n","    branch.dense2.bias.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ni4OTv0ShcU"},"outputs":[],"source":["def freeze_all_layer(wisdomnet):\n","  freeze_DistilBert_layer(wisdomnet.bert)\n","  freeze_branch(wisdomnet.new_branches)"]},{"cell_type":"markdown","metadata":{"id":"2sxhjudr3ELo"},"source":["## Create Custom Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1i-ZKPoA_vhG"},"outputs":[],"source":["class Branch(nn.Module):\n","  def __init__(self, input_size, hidden1_size, hidden2_size, dropout, num_outputs):\n","    super(Branch, self).__init__()\n","\n","    self.dense1 = nn.Linear(input_size, hidden1_size)\n","    self.dropout = nn.Dropout(p=dropout)\n","    self.dense2 = nn.Linear(hidden1_size, hidden2_size)\n","    self.dense3 = nn.Linear(hidden2_size, num_outputs)\n","\n","  def forward(self, x):\n","    out_dense1 = self.dense1(x)\n","    out_dropout = self.dropout(out_dense1)\n","    out_dense2 = self.dense2(out_dropout)\n","    out_dense3 = self.dense3(out_dense2)\n","\n","    return out_dense3"]},{"cell_type":"markdown","metadata":{"id":"5eQWjpjN3QfQ"},"source":["### DistilBert-Escort"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0-Oz9-KWgG6"},"outputs":[],"source":["class BaseModel(nn.Module):\n","    def __init__(self, original_model, num_classes):\n","        super(BaseModel, self).__init__()\n","        self.num_classes = num_classes\n","        self.original_model = original_model\n","        self.branches = nn.ModuleList([Branch(768, 128, 64, 0.1, 2) for _ in range(num_classes)])\n","        self.activation = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n","        out_bert = self.original_model(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_state = out_bert[0]\n","        pooler_out = hidden_state[:, 0]\n","        output_branches = [branch(pooler_out) for branch in self.branches]\n","        # outputs = [self.activation(out_branch) for out_branch in output_branches]\n","\n","        # apply softmax function for each branch\n","        out_soft = [self.activation(out) for out in output_branches]\n","        out_soft_max_indices = [torch.argmax(out, dim=1) for out in out_soft]\n","        out_soft_max_indices = torch.stack(out_soft_max_indices, dim=1)\n","\n","        return out_soft, out_soft_max_indices"]},{"cell_type":"markdown","metadata":{"id":"-qT8CPy93Ud8"},"source":["### WisdomNet Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"miD23gwSxAQI"},"outputs":[],"source":["class WisdomNet(nn.Module):\n","  def __init__(self, trained_model):\n","    super(WisdomNet, self).__init__()\n","    self.numclasses = trained_model.num_classes\n","    self.bert = trained_model.original_model\n","\n","    num_outputs = 3 # add a reject label --> num_outputs = old_outputs + 1\n","    self.new_branches = nn.ModuleList([Branch(768, 128, 64, 0.1, num_outputs) if i == 1 else Branch(768, 128, 64, 0.1, 2) for i in range(self.numclasses)])\n","\n","    for i in range(self.numclasses):\n","      old_branch = trained_model.branches[i]\n","      new_branch = self.new_branches[i]\n","      self.copy_weight_branch(old_branch, new_branch)\n","      new_branch.dense3.weight.data[:2].copy_(old_branch.dense3.weight.data.clone())\n","      new_branch.dense3.bias.data[:2].copy_(old_branch.dense3.bias.data.clone())\n","\n","    self.softmax = nn.Softmax(dim=1)\n","\n","  def copy_weight_branch(self, old_branch, new_branch, name_except='dense3'):\n","    for name, param in old_branch.named_parameters():\n","      if name_except not in name:\n","        name_layer = name.split('.')[0]\n","        name_attr_layer = name.split('.')[1]\n","        new_model_param = getattr(new_branch, name_layer)\n","        if name_attr_layer == 'weight':\n","          new_model_param.weight.data.copy_(param.data.clone())\n","        if name_attr_layer == 'bias':\n","          new_model_param.bias.data.copy_(param.data.clone())\n","\n","  def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n","    out_bert = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","    pooler_out = out_bert.pooler_output\n","    output_branches = [branch(pooler_out) for branch in self.new_branches]\n","\n","    # apply softmax function for each branch\n","    out_soft = [self.softmax(out) for out in output_branches]\n","    out_soft_max_indices = [torch.argmax(out, dim=1) for out in out_soft]\n","    out_soft_max_indices = torch.stack(out_soft_max_indices, dim=1)\n","\n","    return out_soft, out_soft_max_indices"]},{"cell_type":"markdown","metadata":{"id":"K_HaLTY1Q3Yv"},"source":["# Preprocess data"]},{"cell_type":"markdown","metadata":{"id":"YT4g7rCA3c6w"},"source":["## Define Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBpwSctGVSHD"},"outputs":[],"source":["class OpcodeData(Dataset):\n","    def __init__(self, X, y, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.X = X\n","        self.targets = y\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, index):\n","        text = str(self.X[index])\n","\n","        inputs = self.tokenizer(\n","            text,\n","            None,\n","            truncation=True,\n","            padding='max_length',\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","\n","        return {\n","            'index': index,\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[index], dtype=torch.long)\n","        }"]},{"cell_type":"markdown","metadata":{"id":"bgTO9o003har"},"source":["# Define Metrics and Save the result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PbicX7vmZKJA"},"outputs":[],"source":["def calculate_score(y_true, preds, score_type=''):\n","  acc_score = 0.0\n","  if score_type=='multioutput':\n","    num_acc = 0\n","    for i in range(len(y_true)):\n","      if np.array_equal(y_true[i], preds[i]):\n","        num_acc+=1\n","    acc_score = num_acc / len(y_true)\n","  else:\n","    acc_score = accuracy_score(y_true, preds)\n","\n","  return acc_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-XqDi7tjc9st"},"outputs":[],"source":["def save_classification(y_test, y_pred, out_dir, labels):\n","  if isinstance(y_pred, np.ndarray) == False:\n","    y_pred = y_pred.toarray()\n","\n","  def accuracy(y_true, y_pred):\n","    temp = 0\n","    for i in range(y_true.shape[0]):\n","        numerator = sum(np.logical_and(y_true[i], y_pred[i]))\n","        denominator = sum(np.logical_or(y_true[i], y_pred[i]))\n","        if denominator != 0:\n","          temp += numerator / denominator\n","    return temp / y_true.shape[0]\n","\n","  out = classification_report(y_test,y_pred, output_dict=True, target_names=labels)\n","  total_support = out['samples avg']['support']\n","\n","  mr = accuracy_score(y_test, y_pred)\n","  acc = accuracy(y_test,y_pred)\n","  hm = hamming_loss(y_test, y_pred)\n","\n","  out['Exact Match Ratio'] = {'precision': mr, 'recall': mr, 'f1-score': mr, 'support': total_support}\n","  out['Hamming Loss'] = {'precision': hm, 'recall': hm, 'f1-score': hm, 'support': total_support}\n","  out['Accuracy'] = {'precision': acc, 'recall': acc, 'f1-score': acc, 'support': total_support}\n","  out_df = pd.DataFrame(out).transpose()\n","  print(out_df)\n","\n","  out_df.to_csv(out_dir)\n","\n","  return out_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C1WlpXkuY53K"},"outputs":[],"source":["def wisdomnet_cfm(y_true, y_pred, reject_label=2):\n","\n","  if y_true.shape[0] != y_pred.shape[0] or y_true.shape[1] != y_pred.shape[1]:\n","    return Exception(\"Shape is not equally\")\n","\n","  num_samples = y_true.shape[0]\n","  num_label = y_true.shape[1]\n","  cfm = np.zeros(shape=(num_label, 2, 3), dtype=int)\n","\n","  for i in range(num_label):\n","    num_reject = 0\n","    TN, FN, TP, FP = 0, 0, 0, 0\n","    for j in range(num_samples):\n","      if y_pred[j][i] == reject_label:\n","        num_reject+=1\n","      elif y_true[j][i] == 1 and y_pred[j][i] == 1:\n","        TP += 1  # True Positive\n","      elif y_true[j][i] == 0 and y_pred[j][i] == 1:\n","        FP += 1  # False Positive\n","      elif y_true[j][i] == 0 and y_pred[j][i] == 0:\n","        TN += 1  # True Negative\n","      elif y_true[j][i] == 1 and y_pred[j][i] == 0:\n","        FN += 1  # False Negative\n","\n","    cfm[i, 0, 0] = TN\n","    cfm[i, 1, 0] = FN\n","    cfm[i, 1, 1] = TP\n","    cfm[i, 0, 1] = FP\n","    cfm[i, 0, 2] = num_reject\n","    cfm[i, 1, 2] = num_reject\n","\n","  return cfm\n","\n","def calculate_accuracy(y_true, y_pred):\n","  if y_true.shape[0] != y_pred.shape[0] or y_true.shape[1] != y_pred.shape[1]:\n","    return Exception(\"Shape is not equally\")\n","\n","  num_samples = y_true.shape[0]\n","  num_labels = y_true.shape[1]\n","  acc_score = 0.\n","\n","  for i in range(num_samples):\n","    num_acc = 0\n","    total = 0\n","    for j in range(num_labels):\n","      if y_true[i, j] == y_pred[i, j] and y_true[i, j] != 0:\n","        num_acc+=1\n","\n","      if y_true[i, j] != 0 or y_pred[i, j] != 0:\n","        total += 1\n","    local_acc = 0\n","    if total != 0:\n","        local_acc = num_acc / total\n","    acc_score += local_acc\n","  return acc_score / num_samples\n","\n","def my_classification_report(y_true, y_pred, labels):\n","  cfm = wisdomnet_cfm(y_true, y_pred)\n","  num_labels = len(labels)\n","  sum_all_TP, sum_all_FP, sum_all_FN = 0, 0, 0\n","  sum_pre, sum_re, sum_f1 = 0., 0., 0.\n","\n","  report = {}\n","\n","  for i in range(num_labels):\n","    TN = cfm[i, 0, 0]\n","    FN = cfm[i, 1, 0]\n","    TP = cfm[i, 1, 1]\n","    FP = cfm[i, 0, 1]\n","    reject_labels = cfm[i, 0, 2]\n","\n","    label = labels[i]\n","    precision = TP / (TP + FP)\n","    recall = TP / (TP + FN)\n","    F1 = (2 * precision * recall) / (precision + recall)\n","\n","    sum_pre += precision\n","    sum_re += recall\n","    sum_f1 += F1\n","\n","    sum_all_TP += TP\n","    sum_all_FP += FP\n","    sum_all_FN += FN\n","\n","    report[label] = {\n","        'Precision': precision,\n","        'Recall': recall,\n","        'F1': F1,\n","        'Reject Label': reject_labels\n","    }\n","\n","  micro_p = sum_all_TP / (sum_all_TP + sum_all_FP)\n","  micro_r = sum_all_TP / (sum_all_TP + sum_all_FN)\n","  micro_f = (2 * micro_p * micro_r) / (micro_r + micro_p)\n","\n","  report['micro avg'] = {\n","    'Precision': micro_p,\n","    'Recall': micro_r,\n","    'F1': micro_f,\n","    'Reject Label': 0\n","  }\n","\n","  report['macro avg'] = {\n","    'Precision': sum_pre / num_labels,\n","    'Recall': sum_re / num_labels,\n","    'F1': sum_f1 / num_labels,\n","    'Reject Label': 0\n","  }\n","\n","  acc = calculate_accuracy(y_true, y_pred)\n","  report['accuracy'] = {\n","    'Precision': acc,\n","    'Recall': acc,\n","    'F1': acc,\n","    'Reject Label': 0\n","  }\n","\n","  df = pd.DataFrame(report).T\n","  df.columns = ['Precision', 'Recall', 'F1', 'Reject Label']\n","\n","  return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TW4-QSOZr7G1"},"outputs":[],"source":["def get_misclassified_data(labels, preds, indices):\n","  misclassify_data = {}\n","  for i in range(len(labels)):\n","    is_append = False\n","    reject_label = np.array(labels[i])\n","    for j in range(len(labels[i])):\n","      if j == 1 and labels[i, j] != preds[i, j]:\n","        reject_label[j] = 2 # reject label\n","        is_append = True\n","\n","    if is_append:\n","      x_train_index = indices[i]\n","      misclassify_data[x_train_index] = np.array(reject_label)\n","  return misclassify_data"]},{"cell_type":"markdown","metadata":{"id":"uBoGxQWn3pVN"},"source":["# Define Training Fuction"]},{"cell_type":"markdown","metadata":{"id":"YRHK4_Pg3vFY"},"source":["## Train Steps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_jUmUvw3u2k"},"outputs":[],"source":["def train_steps(training_loader, model, loss_f, optimizer, score_type='', is_find_miss=True):\n","    print('Training...')\n","    training_loss = 0\n","    n_correct = 0\n","    nb_tr_steps = 0\n","    nb_tr_examples = 0\n","    train_acc = 0.\n","    train_f1 = 0.\n","    misclassify_train_data = {}\n","\n","    model.train()\n","\n","    for step, batch in enumerate(training_loader):\n","        # push the batch to gpu\n","        indices = batch['index'].numpy()\n","        ids = batch['ids'].to(device)\n","        mask = batch['mask'].to(device)\n","        token_type_ids = batch['token_type_ids'].to(device)\n","        targets = batch['targets'].to(device)\n","\n","        preds, max_indices = model(input_ids=ids, attention_mask=mask)\n","\n","        # calculate the loss for each branch\n","        losses = [loss_f(preds[i], targets[:, i]) for i in range(targets.shape[1])]\n","        average_loss = sum(losses) / targets.shape[1]\n","        training_loss += average_loss.item()\n","\n","        label_ids = targets.to('cpu').numpy()\n","        max_indices = max_indices.detach().cpu().numpy()\n","        acc_score = calculate_score(label_ids, max_indices, score_type)\n","        train_acc += acc_score\n","\n","        if is_find_miss:\n","          misclassify_data = get_misclassified_data(label_ids, max_indices, indices)\n","          misclassify_train_data.update(misclassify_data)\n","\n","        nb_tr_steps += 1\n","\n","        optimizer.zero_grad()\n","        average_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # When using GPU\n","        optimizer.step()\n","\n","    epoch_loss = training_loss / nb_tr_steps\n","    epoch_acc = train_acc / nb_tr_steps\n","\n","    return epoch_loss, epoch_acc, misclassify_train_data"]},{"cell_type":"markdown","metadata":{"id":"eLtUoRo14fxx"},"source":["## Evaluation Steps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YexYuL-A4i7R"},"outputs":[],"source":["def evaluate_steps(validating_loader, model, loss_f, score_type='', **kwargs):\n","    print(\"\\nEvaluating...\")\n","\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","    total_labels = []\n","    # iterate over batches\n","    for step, batch in enumerate(validating_loader):\n","        # push the batch to gpu\n","        ids = batch['ids'].to(device)\n","        mask = batch['mask'].to(device)\n","        token_type_ids = batch['token_type_ids'].to(device)\n","        targets = batch['targets'].to(device)\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","            # model predictions\n","            preds, max_indices = model(input_ids=ids, attention_mask=mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            losses = [loss_f(preds[i], targets[:, i]) for i in range(targets.shape[1])]\n","            average_loss = sum(losses) / targets.shape[1]\n","            total_loss += average_loss.item()\n","\n","            max_indices = max_indices.detach().cpu().numpy()\n","            total_preds += list(max_indices)\n","            total_labels += targets.tolist()\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(validating_loader)\n","    acc_score = calculate_score(total_labels, total_preds, score_type)\n","\n","    return avg_loss, acc_score"]},{"cell_type":"markdown","metadata":{"id":"_D3iiVRE4upf"},"source":["## Predict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnRDGvsa4xtE"},"outputs":[],"source":["def predict(testing_loader, model):\n","    print(\"\\nPredicting...\")\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","    total_labels = []\n","    # iterate over batches\n","    for step, batch in enumerate(testing_loader):\n","        # push the batch to gpu\n","        ids = batch['ids'].to(device)\n","        mask = batch['mask'].to(device)\n","        # token_type_ids = batch['token_type_ids'].to(device)\n","        targets = batch['targets'].to(device)\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","            # model predictions\n","            preds, max_indices = model(input_ids=ids, attention_mask=mask)\n","\n","            max_indices = max_indices.detach().cpu().numpy()\n","            total_preds += list(max_indices)\n","            total_labels += targets.tolist()\n","\n","    return total_labels, total_preds"]},{"cell_type":"markdown","metadata":{"id":"xtVoQSvh4ntk"},"source":["## Train Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lA0PSL4LDZYx"},"outputs":[],"source":["def train_distilbert(epochs, model, optimizer, criterion, dataloader, score_type='', save_dir='distilbert-escort.pt', is_find_miss=True):\n","  data_train_loader, data_val_loader = dataloader\n","  # set initial loss to infinite\n","  best_valid_loss = float('inf')\n","  train_losses = []\n","  valid_losses = []\n","  train_accuracies = []\n","  valid_accuracies = []\n","  misclassify_train_data = {}\n","\n","  for epoch in range(epochs):\n","    print('Epoch {}/{} '.format(epoch + 1, epochs))\n","    start_time = time.time()\n","    train_loss, train_acc, misclassify_train_steps_data = train_steps(data_train_loader, model, criterion, optimizer, score_type=score_type, is_find_miss=is_find_miss)\n","    valid_loss, valid_acc = evaluate_steps(data_val_loader, model, criterion, score_type=score_type)\n","\n","    # save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), save_dir)\n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    train_accuracies.append(train_acc)\n","    valid_accuracies.append(valid_acc)\n","    misclassify_train_data.update(misclassify_train_steps_data)\n","\n","    elapsed_time = time.time() - start_time\n","\n","    print('\\t loss={:.4f} \\t accuracy={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(train_loss, train_acc, valid_loss, valid_acc, elapsed_time))\n","\n","  return train_accuracies, valid_accuracies, train_losses, valid_losses, misclassify_train_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bD7BRKq34rn_"},"outputs":[],"source":["def train_loop_wisdomnet(epochs, model, optimizer, criterion, dataloader, score_type='', save_dir='wisdomnet-secbert-escort.pt', is_find_miss=True):\n","  data_train_loader, data_val_loader, data_test_loader = dataloader\n","  # set initial loss to infinite\n","  best_valid_loss = float('inf')\n","  train_losses = []\n","  valid_losses = []\n","  train_accuracies = []\n","  valid_accuracies = []\n","  misclassify_train_data = {}\n","\n","  for epoch in range(epochs):\n","    print('Epoch {}/{} '.format(epoch + 1, epochs))\n","    start_time = time.time()\n","    train_loss, train_acc, misclassify_train_steps_data = train_steps(data_train_loader, model, criterion, optimizer, score_type=score_type, is_find_miss=is_find_miss)\n","    valid_loss, valid_acc = evaluate_steps(data_val_loader, model, criterion, score_type=score_type)\n","\n","    # save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), str(round(time.time()))+'-'+save_dir)\n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    train_accuracies.append(train_acc)\n","    valid_accuracies.append(valid_acc)\n","    misclassify_train_data.update(misclassify_train_steps_data)\n","\n","    elapsed_time = time.time() - start_time\n","\n","    print('\\t loss={:.4f} \\t accuracy={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(train_loss, train_acc, valid_loss, valid_acc, elapsed_time))\n","\n","    labels = ['Timestamp dependence', 'Outdated Solidity version', 'Frozen Ether', 'Delegatecall Injection']\n","    total_labels, total_preds = predict(data_test_loader, model)\n","    df_report = my_classification_report(np.array(total_labels), np.array(total_preds), labels)\n","\n","    cfm = wisdomnet_cfm(np.array(total_labels), np.array(total_preds))\n","    print(cfm)\n","    TN = cfm[1, 0, 0]\n","    FN = cfm[1, 1, 0]\n","    TP = cfm[1, 1, 1]\n","    FP = cfm[1, 0, 1]\n","    num_reject = cfm[1, 0, 2]\n","    true_sample = TP / 118863.0\n","    false_sample = FN / 118863.0\n","    reject_sample = num_reject / 118863.0\n","    print('\\t true_sample={:.4f} \\t false_sample={:.4f} \\t reject_sample={:.4f} '.format(true_sample, false_sample, reject_sample))\n","\n","  return train_accuracies, valid_accuracies, train_losses, valid_losses, misclassify_train_data"]},{"cell_type":"markdown","metadata":{"id":"0zS2ROZ14zNv"},"source":["## Draw the graph"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mt0cnLwyo1lz"},"outputs":[],"source":["def plot_graph(epochs, train, valid, tittle):\n","    fig = plt.figure(figsize=(12,12))\n","    plt.title(tittle)\n","    plt.plot(list(np.arange(epochs) + 1) , train, label='train')\n","    plt.plot(list(np.arange(epochs) + 1), valid, label='validation')\n","    plt.xlabel('num_epochs', fontsize=12)\n","    plt.ylabel('loss', fontsize=12)\n","    plt.legend(loc='best')"]},{"cell_type":"markdown","metadata":{"id":"pv9i798w45Bz"},"source":["# Run model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhFpZdsnEZlL"},"outputs":[],"source":["# Defining some key variables that will be used later on in the training\n","max_length = 512\n","TRAIN_BATCH_SIZE = 64\n","VALID_BATCH_SIZE = 64\n","EPOCHS = 50\n","LEARNING_RATE = 1e-04\n","num_class = 4\n","# data_folder = os.getcwd()+'/Wisdomnet/Untitled Folder/'\n","data_folder = '/home/tzt5387/Desktop/Untitled Folder/'\n","labels = ['Timestamp dependence', 'Outdated Solidity version', 'Frozen Ether', 'Delegatecall Injection']"]},{"cell_type":"markdown","metadata":{"id":"PDYIGI3ECv80"},"source":["## Train DistilBert ESCORT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLXOMbYRE7fw"},"outputs":[],"source":["# os.listdir(os.getcwd()+'/Wisdomnet/Untitled Folder')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ox-JuOwOFmwi"},"outputs":[],"source":["X_train = pd.read_csv(data_folder+'X_train.csv').to_numpy()\n","X_test = pd.read_csv(data_folder+'X_test.csv').to_numpy()\n","X_val = pd.read_csv(data_folder+'X_val.csv').to_numpy()\n","\n","y_train = pd.read_csv(data_folder+'y_train.csv').to_numpy()\n","y_test = pd.read_csv(data_folder+'y_test.csv').to_numpy()\n","y_val = pd.read_csv(data_folder+'y_val.csv').to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n5zTyWlzFdv8"},"outputs":[],"source":["training_set = OpcodeData(X_train, y_train, distilBertTokenizer, max_length)\n","validating_set = OpcodeData(X_val, y_val, distilBertTokenizer, max_length)\n","testing_set = OpcodeData(X_test, y_test, distilBertTokenizer, max_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uU-47zwVGDdF"},"outputs":[],"source":["# Create generator for Dataset with BATCH_SIZE\n","training_loader = DataLoader(training_set, shuffle=True, batch_size=TRAIN_BATCH_SIZE)\n","validating_loader = DataLoader(validating_set, shuffle=True, batch_size=VALID_BATCH_SIZE)\n","testing_loader = DataLoader(testing_set, batch_size=VALID_BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Hx0jw5-CveX"},"outputs":[],"source":["distilBertMultiLabelModel = BaseModel(original_model=distilBertModel, num_classes=4)\n","freeze_DistilBert_layer(distilBertMultiLabelModel)\n","distilBertMultiLabelModel = nn.DataParallel(distilBertMultiLabelModel)\n","distilBertMultiLabelModel.to(device)\n","\n","# Creating the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params = distilBertMultiLabelModel.parameters(), lr=LEARNING_RATE)\n","\n","train_accuracies, valid_accuracies, train_losses, valid_losses, misclassify_train_data = train_distilbert(EPOCHS, distilBertMultiLabelModel, optimizer, criterion, (training_loader, validating_loader))\n","df = pd.DataFrame.from_dict(misclassify_train_data, orient='index', columns=labels)\n","df.index.name = 'X_train_index'\n","df.to_csv(data_folder+'misclassified-data.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQ2GsQ4SG3Ic"},"outputs":[],"source":["\"\"\"\n","Plot the result of training process\n","\"\"\"\n","plot_graph(EPOCHS, train_losses, valid_losses, \"Train/Validation Loss\")\n","plot_graph(EPOCHS, train_accuracies, valid_accuracies, \"Train/Validation Accuracy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4J0q6bntG6yp"},"outputs":[],"source":["\"\"\"\n","Evaluate model on test set and save the result\n","\"\"\"\n","total_labels, total_preds = predict(testing_loader, distilBertMultiLabelModel)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTUTCJ3vG-ft"},"outputs":[],"source":["df_labels = pd.DataFrame(total_labels, columns=labels)\n","df_preds = pd.DataFrame(total_preds, columns=labels)\n","\n","df_labels.to_csv(data_folder+'labels-test-distil-bert-escort.csv')\n","df_preds.to_csv(data_folder+'preds-distil-bert-escort.csv')\n","\n","save_classification(y_test=np.array(total_labels), y_pred=np.array(total_preds), labels=labels, out_dir='distil-bert.csv')"]},{"cell_type":"markdown","metadata":{"id":"vizEvDsHPnWv"},"source":["## Misclassified Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jcq4n7fAWq9A"},"outputs":[],"source":["def misclassified_analysis(misclassified_df):\n","  values = []\n","  keys = [0 ,1, 2]\n","  labels = misclassified_df.columns[-4:]\n","  for i in labels:\n","    value_counts = misclassified_df[i].value_counts().to_dict()\n","    value = [0, 0, 0]\n","    for key in keys:\n","      if key in value_counts.keys():\n","        value[key] = value_counts[key]\n","      else:\n","        value[key] = 0\n","\n","    values.append(value)\n","\n","  values = np.array(values).T\n","  values.shape[0]\n","  print(values)\n","  print(np.sum(values, axis=0))\n","\n","  # Plot\n","  fig, ax = plt.subplots(figsize=(16, 9))\n","  name = ['No', 'Yes', 'Reject']\n","  # Stacked bar chart\n","  for i in range(values.shape[0]):\n","    ax.bar(labels, values[i], bottom = np.sum(values[:i], axis = 0), width = 0.5, label = str(name[i]))\n","\n","  for bar in ax.patches:\n","    ax.text(bar.get_x() + bar.get_width() / 2,\n","            bar.get_height() / 2 + bar.get_y(),\n","            round(bar.get_height()), ha = 'center',\n","            color = 'black', weight = 'bold', size = 10)\n","\n","  plt.legend()\n","  ax.set_ylabel('Total')\n","  ax.set_xlabel('Name of vulnerbilities')\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6MlY7qJRMyO"},"outputs":[],"source":["df_y_train = pd.read_csv(os.getcwd()+'/Wisdomnet/Untitled Folder/y_train.csv')\n","df_y_val = pd.read_csv(os.getcwd()+'/Wisdomnet/Untitled Folder/y_val.csv')\n","df_y_test = pd.read_csv(os.getcwd()+'/Wisdomnet/Untitled Folder/y_test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0rw5B68RX2g"},"outputs":[],"source":["misclassified_analysis(df_y_train)\n","misclassified_analysis(df_y_val)\n","misclassified_analysis(df_y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6IwiIhXQxMA"},"outputs":[],"source":["os.listdir(os.getcwd()+'/Wisdomnet/Untitled Folder/y_train.csv')"]},{"cell_type":"markdown","metadata":{"id":"U0CPPSe05ILv"},"source":["## Train wisdomnet Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7mwa8iB93fpL"},"outputs":[],"source":["def get_mis_data(misclassified_df, frac=0.01):\n","  # df = pd.DataFrame(columns=misclassified_df.columns)\n","  # labels = misclassified_df.columns[-4:]\n","  # reject_label = 2\n","\n","  # def find_min_reject(misclassified_df):\n","  #   num_reject_counts = misclassified_df.apply(lambda x: (x == 2).sum())\n","  #   min_num_reject = num_reject_counts.min()\n","  #   min_index = num_reject_counts.argmin()\n","  #   min_label = num_reject_counts.index[min_index]\n","\n","  #   return min_label, min_num_reject\n","\n","  # min_label, min_num_reject = find_min_reject(misclassified_df.iloc[:, -4:])\n","  # threshold = int(min_num_reject * frac)\n","\n","  # for label in labels:\n","  #   df_filtered = misclassified_df.loc[misclassified_df[label] == reject_label, :]\n","  #   df = pd.concat([df, df_filtered[:threshold]])\n","\n","  df = misclassified_df.sample(frac=frac, random_state=2023)\n","  return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dCxXx_s93iFt"},"outputs":[],"source":["def train_wisdomnet(frac_mis=0.01):\n","    print('Load trained model')\n","    distilBertMultiLabelModel = BaseModel(original_model=distilBertModel, num_classes=4)\n","    distilBertMultiLabelModel.load_state_dict(torch.load(os.getcwd()+'/secbert-escort.pt'))\n","    freeze_all_layer(distilBertMultiLabelModel)\n","\n","    wisdomnet = WisdomNet(distilBertMultiLabelModel)\n","    freeze_all_layer(wisdomnet)\n","    wisdomnet.to(device)\n","\n","    # data = pd.read_csv(data_folder + '/Data_Cleansing_a.csv')\n","    print('Load misclassified outdated data')\n","    misclassified_df = pd.read_csv(os.getcwd()+'/data-multilabel/misclassified-outdate.csv')\n","    mis_df = get_mis_data(misclassified_df, frac=frac_mis)\n","    misclassified_analysis(mis_df)\n","    X_m_index = mis_df['X_train_index'].to_list()\n","    X_train = pd.read_csv(os.getcwd()+'/data-multilabel/X_train.csv')\n","\n","    X_m, y_m = X_train.iloc[X_m_index].to_numpy(), np.array(mis_df.iloc[:, -4:].to_numpy(), dtype='int64')\n","    X_m_train, X_m_test, y_m_train, y_m_test = train_test_split(X_m, y_m, test_size=0.2, random_state=2023)\n","    X_m_train, X_m_val, y_m_train, y_m_val = train_test_split(X_m_train, y_m_train, test_size=0.2, random_state=2023)\n","\n","    training_m_set = OpcodeData(X_m_train, y_m_train, distilBertTokenizer, max_length)\n","    validating_m_set = OpcodeData(X_m_val, y_m_val, distilBertTokenizer, max_length)\n","    testing_m_set = OpcodeData(X_m_test, y_m_test, distilBertTokenizer, max_length)\n","\n","    # Create generator for Dataset with BATCH_SIZE\n","    training_m_loader = DataLoader(training_m_set, batch_size=TRAIN_BATCH_SIZE)\n","    validating_m_loader = DataLoader(validating_m_set, batch_size=VALID_BATCH_SIZE)\n","    testing_m_loader = DataLoader(testing_m_set, batch_size=VALID_BATCH_SIZE)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(params = wisdomnet.parameters(), lr=LEARNING_RATE)\n","\n","    print('Train Wisdom Net Model')\n","    train_m_accuracies, valid_m_accuracies, train_m_losses, valid_m_losses, _ = train_loop_wisdomnet(EPOCHS, wisdomnet, optimizer, criterion, (training_m_loader, validating_m_loader, testing_m_loader), score_type='multioutput', save_dir='wisdomnet-escort'+str(frac_mis*100)+'-percent.pt', is_find_miss=False)\n","\n","    \"\"\"\n","    Plot the result of training process\n","    \"\"\"\n","    plot_graph(EPOCHS, train_m_losses, valid_m_losses, \"Train/Validation Loss\")\n","    plot_graph(EPOCHS, train_m_accuracies, valid_m_accuracies, \"Train/Validation Accuracy\")\n","\n","    \"\"\"\n","    Evaluate model on test set and save the result\n","    \"\"\"\n","    print('Test Wisdom Net Model')\n","    X_test = pd.read_csv(os.getcwd()+'/data-multilabel/X_test.csv').to_numpy()\n","    y_test = pd.read_csv(os.getcwd()+'/data-multilabel/y_test.csv').to_numpy()\n","\n","    testing_set = OpcodeData(X_test, y_test, distilBertTokenizer, max_length)\n","    testing_loader = DataLoader(testing_set, batch_size=VALID_BATCH_SIZE)\n","\n","    labels = ['Timestamp dependence', 'Outdated Solidity version', 'Frozen Ether', 'Delegatecall Injection']\n","    start = time.time()\n","    total_labels, total_preds = predict(testing_loader, wisdomnet)\n","    end = time.time()\n","\n","    execution_time = (end - start) / len(total_labels)\n","    print('Execution time: ', execution_time)\n","    df_labels = pd.DataFrame(total_labels, columns=labels)\n","    df_preds = pd.DataFrame(total_preds, columns=labels)\n","\n","    print(wisdomnet_cfm(df_labels.to_numpy(), df_preds.to_numpy()))\n","    mycr = my_classification_report(df_labels.to_numpy(), df_preds.to_numpy(), labels)\n","    mycr.to_csv('./'+str(frac_mis*100)+'-percent.csv')\n","    print(mycr)\n","\n","#     df_labels.to_csv('./'+str(frac_mis*100)+'-percent.csv')\n","#     df_preds.to_csv('./'+str(frac_mis*100)+'-percent.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Roml1Ji2ozEu"},"outputs":[],"source":["# train_wisdomnet(frac_mis=0.01)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SmOV-8S1ozEw"},"outputs":[],"source":["# train_wisdomnet(frac_mis=0.05)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5xT9lM-ozEy"},"outputs":[],"source":["# train_wisdomnet(frac_mis=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MW-dmFSQozEz"},"outputs":[],"source":["# train_wisdomnet(frac_mis=0.15)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"radS2D3lozE0"},"outputs":[],"source":["# train_wisdomnet(frac_mis=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AGdeoN3ozE2"},"outputs":[],"source":["# train_wisdomnet(frac_mis=0.3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttVYAsEe1cyQ"},"outputs":[],"source":["# train_wisdomnet(frac_mis=0.4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkcE6BRNozE3"},"outputs":[],"source":["# train_wisdomnet(frac_mis=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_b86q-BQozE4"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["vizEvDsHPnWv"],"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}