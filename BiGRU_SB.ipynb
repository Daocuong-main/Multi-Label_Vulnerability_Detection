{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHzmrdy0f8Dk"
   },
   "source": [
    "This code implement the architecture for multilabel text classification in [paper](https://arxiv.org/pdf/2103.12607v1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2lOX1Sr0iSD"
   },
   "source": [
    "# Import and connect to gg drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29907,
     "status": "ok",
     "timestamp": 1694228488057,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "diMysT7u0l9M",
    "outputId": "a3482555-2342-4ceb-c52b-6b30c7c06794"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1694228488058,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "q-E2Mi_30otZ",
    "outputId": "57b62771-929a-46e1-9452-02a36fb11d81"
   },
   "outputs": [],
   "source": [
    "# cd /content/drive/MyDrive/lab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7447,
     "status": "ok",
     "timestamp": 1694228495500,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "_fJlPHWD0qcl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1694228495501,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "ZlwZK-zl-Lfr",
    "outputId": "0eed464a-7619-4afb-f6f6-57974e1cf8f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    " dev = \"cuda\"\n",
    "else:\n",
    " dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1694228495501,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "cqJSks2dp6QU"
   },
   "outputs": [],
   "source": [
    "def save_classification(y_test, y_pred, out_dir, labels):\n",
    "  if isinstance(y_pred, np.ndarray) == False:\n",
    "    y_pred = y_pred.toarray()\n",
    "\n",
    "  def accuracy(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        numerator = sum(np.logical_and(y_true[i], y_pred[i]))\n",
    "        denominator = sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "        if denominator != 0:\n",
    "          temp += numerator / denominator\n",
    "    return temp / y_true.shape[0]\n",
    "\n",
    "  out = classification_report(y_test,y_pred, output_dict=True, target_names=labels)\n",
    "  total_support = out['samples avg']['support']\n",
    "\n",
    "  mr = accuracy_score(y_test, y_pred)\n",
    "  acc = accuracy(y_test,y_pred)\n",
    "  hm = hamming_loss(y_test, y_pred)\n",
    "\n",
    "  out['Exact Match Ratio'] = {'precision': mr, 'recall': mr, 'f1-score': mr, 'support': total_support}\n",
    "  out['Hamming Loss'] = {'precision': hm, 'recall': hm, 'f1-score': hm, 'support': total_support}\n",
    "  out['Accuracy'] = {'precision': acc, 'recall': acc, 'f1-score': acc, 'support': total_support}\n",
    "  out_df = pd.DataFrame(out).transpose()\n",
    "  print(out_df)\n",
    "\n",
    "  out_df.to_csv(out_dir)\n",
    "\n",
    "  return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAQOzGAG1XRr"
   },
   "source": [
    "# Extract and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 61609,
     "status": "ok",
     "timestamp": 1694228557106,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "jvjJgMdH1KTP"
   },
   "outputs": [],
   "source": [
    "data_folder = os.getcwd()+'/Untitled Folder/'\n",
    "\n",
    "# def extract_file():\n",
    "#   file_zip = os.getcwd() + '/Data_Cleansing.zip'\n",
    "#   with zipfile.ZipFile(file_zip, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(path=data_folder)\n",
    "\n",
    "# extract_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 74596,
     "status": "ok",
     "timestamp": 1694228631680,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "DETohp6i2trU"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(data_folder+'X_train.csv')['BYTECODE'].to_numpy()\n",
    "X_test = pd.read_csv(data_folder+'X_test.csv')['BYTECODE'].to_numpy()\n",
    "X_val = pd.read_csv(data_folder+'X_val.csv')['BYTECODE'].to_numpy()\n",
    "\n",
    "y_train = pd.read_csv(data_folder+'y_train.csv').to_numpy()\n",
    "y_test = pd.read_csv(data_folder+'y_test.csv').to_numpy()\n",
    "y_val = pd.read_csv(data_folder+'y_val.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8wki3TAqGFYM"
   },
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, num_words=None, lower=True) -> None:\n",
    "        self.word_index = {}\n",
    "        self.word_counts = {}\n",
    "        self.num_words = num_words\n",
    "        self.split = \" \"\n",
    "        self.lower = lower\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        \"\"\"\n",
    "        create vocabulary\n",
    "\n",
    "        Args:\n",
    "            text: list of strings or list of list of strings\n",
    "        \"\"\"\n",
    "        for text in texts:\n",
    "            seq = self.text_to_word_sequence(text)\n",
    "            for w in seq:\n",
    "                if w in self.word_counts:\n",
    "                    self.word_counts[w] += 1\n",
    "\n",
    "                else:\n",
    "                    self.word_counts[w] = 1\n",
    "        vocab = self.word_counts.keys()\n",
    "        self.word_index = dict(zip(vocab, list(range(1, len(vocab) + 1))))\n",
    "\n",
    "    def text_to_word_sequence(self, input_text):\n",
    "        if self.lower == True:\n",
    "            input_text = input_text.lower()\n",
    "\n",
    "        seq = input_text.split(self.split)\n",
    "        return seq\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        return list(self.texts_to_sequences_generator(texts))\n",
    "\n",
    "    def texts_to_sequences_generator(self, texts):\n",
    "        for text in texts:\n",
    "            seq = self.text_to_word_sequence(text)\n",
    "            vect = []\n",
    "            for w in seq:\n",
    "                i = self.word_index.get(w)\n",
    "                vect.append(i)\n",
    "            yield vect\n",
    "\n",
    "def pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=None,\n",
    "    dtype=\"int32\",\n",
    "    padding=\"pre\",\n",
    "    truncating=\"pre\",\n",
    "    value=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: List of sequences (each sequence is a list of integers).\n",
    "        maxlen: Optional Int, maximum length of all sequences. If not provided,\n",
    "            sequences will be padded to the length of the longest individual\n",
    "            sequence.\n",
    "        dtype: (Optional, defaults to `\"int32\"`). Type of the output sequences.\n",
    "            To pad sequences with variable length strings, you can use `object`.\n",
    "        padding: String, \"pre\" or \"post\" (optional, defaults to `\"pre\"`):\n",
    "            pad either before or after each sequence.\n",
    "        truncating: String, \"pre\" or \"post\" (optional, defaults to `\"pre\"`):\n",
    "            remove values from sequences larger than\n",
    "            `maxlen`, either at the beginning or at the end of the sequences.\n",
    "        value: Float or String, padding value. (Optional, defaults to 0.)\n",
    "\n",
    "    Returns:\n",
    "        Numpy array with shape `(len(sequences), maxlen)`\n",
    "\n",
    "    Raises:\n",
    "        ValueError: In case of invalid values for `truncating` or `padding`,\n",
    "            or in case of invalid shape for a `sequences` entry.\n",
    "    \"\"\"\n",
    "\n",
    "    if not hasattr(sequences, \"__len__\"):\n",
    "        raise ValueError(\"`sequences` must be iterable.\")\n",
    "    num_samples = len(sequences)\n",
    "\n",
    "    lengths = []\n",
    "    sample_shape = ()\n",
    "    flag = True\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "\n",
    "    for x in sequences:\n",
    "        try:\n",
    "            lengths.append(len(x))\n",
    "            if flag and len(x):\n",
    "                sample_shape = np.asarray(x).shape[1:]\n",
    "                flag = False\n",
    "        except TypeError as e:\n",
    "            raise ValueError(\n",
    "                \"`sequences` must be a list of iterables. \"\n",
    "                f\"Found non-iterable: {str(x)}\"\n",
    "            ) from e\n",
    "\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(\n",
    "        dtype, np.unicode_\n",
    "    )\n",
    "    if isinstance(value, str) and dtype != object and not is_dtype_str:\n",
    "        raise ValueError(\n",
    "            f\"`dtype` {dtype} is not compatible with `value`'s type: \"\n",
    "            f\"{type(value)}\\nYou should set `dtype=object` for variable length \"\n",
    "            \"strings.\"\n",
    "        )\n",
    "\n",
    "    x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if not len(s):\n",
    "            continue  # empty list/array was found\n",
    "        if truncating == \"pre\":\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == \"post\":\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError(f'Truncating type \"{truncating}\" not understood')\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError(\n",
    "                f\"Shape of sample {trunc.shape[1:]} of sequence at \"\n",
    "                f\"position {idx} is different from expected shape \"\n",
    "                f\"{sample_shape}\"\n",
    "            )\n",
    "\n",
    "        if padding == \"post\":\n",
    "            x[idx, : len(trunc)] = trunc\n",
    "        elif padding == \"pre\":\n",
    "            x[idx, -len(trunc) :] = trunc\n",
    "        else:\n",
    "            raise ValueError(f'Padding type \"{padding}\" not understood')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "336ITdh_HNIW"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts=X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(texts=X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts=X_test)\n",
    "sequences_val = tokenizer.texts_to_sequences(texts=X_val)\n",
    "input_size = 4100\n",
    "\n",
    "X_train = pad_sequences(sequences_train, maxlen=input_size)\n",
    "X_val = pad_sequences(sequences_val, maxlen=input_size)\n",
    "X_test = pad_sequences(sequences_test, maxlen=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122280, 4100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF9kUKTQ23ok"
   },
   "source": [
    "# Multilabel deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1q9IdP92n3xT"
   },
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yQHSbxdc_Ewh"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "tensor_X_train = torch.tensor(X_train)\n",
    "tensor_X_val = torch.tensor(X_val)\n",
    "tensor_X_test = torch.tensor(X_test)\n",
    "tensor_Y_train = torch.FloatTensor(y_train)\n",
    "tensor_Y_val = torch.FloatTensor(y_val)\n",
    "tensor_Y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "train_dataset = TensorDataset(tensor_X_train, tensor_Y_train)\n",
    "val_dataset = TensorDataset(tensor_X_val, tensor_Y_val)\n",
    "test_dataset = TensorDataset(tensor_X_test, tensor_Y_test)\n",
    "\n",
    "data_train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "data_val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "data_test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BbYNVPGPu1gC"
   },
   "outputs": [],
   "source": [
    "class Branch(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, dropout, num_outputs):\n",
    "    super(Branch, self).__init__()\n",
    "\n",
    "    self.dense1 = nn.Linear(input_size, hidden_size)\n",
    "    self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "    self.dense2 = nn.Linear(hidden_size, num_outputs)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # print(\"Branch Input Shape:\", x.shape)\n",
    "    out_dense1 = self.dense1(x)\n",
    "    # print(\"After Dense1 Shape:\", out_dense1.shape)\n",
    "    out_batchnorm1 = self.batchnorm1(out_dense1)\n",
    "    out_dropout = self.dropout(out_batchnorm1)\n",
    "    out_dense2 = self.dense2(out_dropout)\n",
    "\n",
    "    return out_dense2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvxJELKhn7ny"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "N95V8a_125ys"
   },
   "outputs": [],
   "source": [
    "class Escort(nn.Module):\n",
    "  def __init__(self, vocab_size, embedd_size, gru_hidden_size, n_layers, num_classes):\n",
    "    super(Escort, self).__init__()\n",
    "    self.gru_hidden_size = gru_hidden_size\n",
    "    self.word_embeddings = nn.Embedding(vocab_size, embedd_size, padding_idx=0)\n",
    "    # Set bidirectional=True to use a Bi-GRU\n",
    "    self.gru = nn.GRU(embedd_size, gru_hidden_size, num_layers=n_layers, batch_first=True, bidirectional=True)\n",
    "    # The output size of the Bi-GRU is twice the hidden size\n",
    "    self.branches = Branch(2 * gru_hidden_size, 128, 0.2, num_classes)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, sequence):\n",
    "    # print(\"Input to Escort:\", sequence.shape)\n",
    "    embeds = self.word_embeddings(sequence)\n",
    "    # print(\"After Embedding Shape:\", embeds.shape)\n",
    "    gru_out, _ = self.gru(embeds)\n",
    "    # print(\"GRU Output Shape:\", gru_out.shape)\n",
    "    # Concatenate the last hidden states of both directions\n",
    "    output_branch = self.branches(torch.cat((gru_out[:, -1, :self.gru_hidden_size], gru_out[:, 0, self.gru_hidden_size:]), dim=1))\n",
    "    outputs = self.sigmoid(output_branch)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BAJUkZcoAyE"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NW7noauGLMtU"
   },
   "outputs": [],
   "source": [
    "# model = LSTMMultilabel(SIZE_OF_VOCAB, NUM_HIDDEN_NODES, NUM_OUTPUT_NODES, NUM_LAYERS, DROPOUT, True, torch.DoubleTensor(weight))\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zBfVRGCGMQxh"
   },
   "outputs": [],
   "source": [
    "# inputs, labels = next(iter(data_train_loader))\n",
    "# preds = model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vxHiIbwrKRS"
   },
   "source": [
    "### Train and Validation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UF8lqT-M-BL6"
   },
   "outputs": [],
   "source": [
    "def calculate_score(y_true, preds):\n",
    "    acc_score = accuracy_score(y_true, preds)\n",
    "\n",
    "    return acc_score\n",
    "\n",
    "def train_steps(training_loader, model, loss_f, optimizer):\n",
    "    training_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    train_acc = 0.\n",
    "\n",
    "    model.train()\n",
    "    for step, batch in enumerate(training_loader):\n",
    "        # push the batch to gpu\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "\n",
    "        preds = model(inputs)\n",
    "\n",
    "        loss = loss_f(preds, labels)\n",
    "        training_loss += loss.item()\n",
    "\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        preds = np.where(preds>=0.5, 1, 0)\n",
    "        labels = labels.to('cpu').numpy()\n",
    "\n",
    "        acc_score = calculate_score(labels, preds)\n",
    "        train_acc += acc_score\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = training_loss / nb_tr_steps\n",
    "    epoch_acc = train_acc / nb_tr_steps\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_steps(validating_loader, model, loss_f):\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(validating_loader):\n",
    "        # push the batch to gpu\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds = model(inputs)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = loss_f(preds, labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            preds = np.where(preds>=0.5, 1, 0)\n",
    "            total_preds += list(preds)\n",
    "            total_labels += labels.tolist()\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(validating_loader)\n",
    "    acc_score = calculate_score(total_labels, total_preds)\n",
    "\n",
    "    return avg_loss, acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZ9uzmMGsOhR"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rQ82Aqtbpifd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(epochs, model, optimizer, criterion):\n",
    "  # empty lists to store training and validation loss of each epoch\n",
    "  # set initial loss to infinite\n",
    "  best_valid_loss = float('inf')\n",
    "  train_losses = []\n",
    "  valid_losses = []\n",
    "  train_accuracies = []\n",
    "  valid_accuracies = []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_steps(data_train_loader, model, criterion, optimizer)\n",
    "\n",
    "    valid_loss, valid_acc = evaluate_steps(data_val_loader, model, criterion)\n",
    "\n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'trained_gru_escort.pt')\n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    valid_accuracies.append(valid_acc)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print('Epoch {}/{} \\t loss={:.4f} \\t accuracy={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(epoch + 1, epochs, train_loss, train_acc, valid_loss, valid_acc, elapsed_time))\n",
    "  return train_accuracies, valid_accuracies, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yNXt80cVsWgx"
   },
   "outputs": [],
   "source": [
    "def plot_graph(epochs, train, valid, tittle):\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    plt.title(tittle)\n",
    "    plt.plot(list(np.arange(epochs) + 1) , train, label='train')\n",
    "    plt.plot(list(np.arange(epochs) + 1), valid, label='validation')\n",
    "    plt.xlabel('num_epochs', fontsize=12)\n",
    "    plt.ylabel('loss', fontsize=12)\n",
    "    plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXnRPRFDwxle"
   },
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "soXxXY8yw3Er"
   },
   "outputs": [],
   "source": [
    "def predict(testing_loader, model, loss_f):\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    start_time = time.time()\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(testing_loader):\n",
    "        # push the batch to gpu\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds = model(inputs)\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            preds = np.where(preds>=0.5, 1, 0)\n",
    "            total_preds += list(preds)\n",
    "            total_labels += labels.tolist()\n",
    "\n",
    "    execution_time = (time.time() - start_time) / len(total_labels)\n",
    "    return total_preds, total_labels, execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_sample(testing_loader, model):\n",
    "    print(\"\\nPredicting...\")\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    # get the first image and label from the testing loader\n",
    "    batch= next(iter(testing_loader))\n",
    "    # push the inputs and label to gpu\n",
    "    inputs = batch[0].to(device)\n",
    "    labels = batch[1].to(device)\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "        # model predictions\n",
    "        start_time_k = time.time()\n",
    "        preds = model(inputs)\n",
    "        end_time_k = time.time()\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        c = np.where(preds>=0.5, 1, 0)\n",
    "        # get the first prediction and label\n",
    "        pred = preds[0]\n",
    "        label = labels[0][0].item()\n",
    "    # calculate the runtime\n",
    "    runtime = end_time_k - start_time_k\n",
    "    return label, pred, runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9H2Jpz4Qx0MC"
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1694061109754,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "kr8ZtPtzx1VW",
    "outputId": "33abc99f-a97c-4aec-f38f-10c02df15b1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290\n",
      "Escort(\n",
      "  (word_embeddings): Embedding(290, 5, padding_idx=0)\n",
      "  (gru): GRU(5, 128, batch_first=True, bidirectional=True)\n",
      "  (branches): Branch(\n",
      "    (dense1): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (batchnorm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (dense2): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "SIZE_OF_VOCAB = len(tokenizer.word_index.keys())\n",
    "print(SIZE_OF_VOCAB)\n",
    "EMBEDDED_SIZE = 5\n",
    "GRU_HIDDEN_SIZE = 128\n",
    "NUM_OUTPUT_NODES = 4\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.2\n",
    "model = Escort(SIZE_OF_VOCAB, EMBEDDED_SIZE, GRU_HIDDEN_SIZE, NUM_LAYERS, NUM_OUTPUT_NODES)\n",
    "# model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64510,
     "status": "ok",
     "timestamp": 1694061177086,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "vTM0v9sK3nCZ",
    "outputId": "fac1f7c2-b792-47c0-e0da-37bbb86a8841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 \t loss=0.1870 \t accuracy=0.7385 \t val_loss=0.1723  \t val_acc=0.7444  \t time=107.21s\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "train_accuracies, valid_accuracies, train_losses, valid_losses = train(epochs, model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 892,
     "status": "ok",
     "timestamp": 1694061194264,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "n0ftnFQa3k34",
    "outputId": "5a83872f-22f8-4c9f-db11-613240211356"
   },
   "outputs": [],
   "source": [
    "# plot_graph(epochs, train_losses, valid_losses, \"GRU_Train_Validation_Loss\")\n",
    "# plot_graph(epochs, train_accuracies, valid_accuracies, \"GRU_Train_Validation_Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "pELUFirP3Kcx"
   },
   "outputs": [],
   "source": [
    "# total_preds, total_labels, execution_time = predict(data_test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1694061194786,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "SkKf6FVD5NVW",
    "outputId": "c8bb6973-b009-4e6f-b387-9c56a46e1d85"
   },
   "outputs": [],
   "source": [
    "# execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting...\n",
      "Predicting took 0.003076791763305664 seconds.\n"
     ]
    }
   ],
   "source": [
    "_, _, elapsed_time = predict_one_sample(data_test_loader, model) # call predict and store the results\n",
    "print(f\"Predicting took {elapsed_time} seconds.\") # print the elapsed time in a formatted way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# labels = ['Timestamp dependence', 'Outdated Solidity version', 'Frozen Ether', 'Delegatecall Injection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1694061196096,
     "user": {
      "displayName": "tuan nguyen",
      "userId": "06533742515184919841"
     },
     "user_tz": -420
    },
    "id": "KoxRi-3x4JGY",
    "outputId": "24e519a0-38ee-4156-cd39-8e2a3f532d49",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_classification(y_pred=np.array(total_preds), y_test=np.array(total_labels), labels=labels, out_dir='escort.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
